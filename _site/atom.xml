<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>skoo's notes</title>
 <link href="http://skoo87.github.io/" rel="self"/>
 <link href="http://skoo87.github.io"/>
 <updated>2014-04-30T22:51:04+08:00</updated>
 <id>http://skoo87.github.io</id>
 <author>
   <name>skoo</name>
   <email>marckywu@gmail.com</email>
 </author>

 
 <entry>
   <title>Heka插件开发</title>
   <link href="http://skoo87.github.io/system/2014/04/30/heka-plugin-devel"/>
   <updated>2014-04-30T00:00:00+08:00</updated>
   <id>http://skoo87.github.io/system/2014/04/30/heka-plugin-devel</id>
   <content type="html">&lt;p&gt;Heka是一个实时数据收集、处理和分析的工具，具备高可扩展的插件开发能力。本文是自己调研Heka插件开发的一个总结，方便快速入门插件开发。&lt;/p&gt;

&lt;p&gt;2014 gophercon上关于Heka的演讲Slide：&lt;a href='https://cdn.rawgit.com/gophercon/2014-talks/master/rob_miller_heka/index.html#/'&gt;https://cdn.rawgit.com/gophercon/2014-talks/master/rob_miller_heka/index.html#/&lt;/a&gt;，从这个Slide中借用一张Heka内部架构图：&lt;/p&gt;
&lt;div align='center'&gt;
&lt;img height='300' src='/assets/images/heka-overview-diagram.png' width='600' /&gt;
&lt;/div&gt;
&lt;p&gt;内部架构图清晰的反应出了数据从进入Heka到流出Heka的整个过程需要经历一些什么样的组件。图中的箭头符号反应出了，一个数据进入Heka后可以选择什么样的路径，路径并不是唯一的，一切都可以根据需求来设置。&lt;/p&gt;

&lt;p&gt;内部架构图中展示的所有组件，我们可以通过开发插件定制的部分分别是：Inputs、Decoders、Filters和Outputs。&lt;/p&gt;
&lt;br /&gt;
&lt;h4 id='id19'&gt;编译源码&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;从github上克隆出Heka源码库&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/mozilla-services/heka&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;

&lt;li&gt;
&lt;p&gt;查看Heka已经release的版本，其实就是打的tag&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git tag&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我看到的最新release版本是v0.5.1，因此我们选择这个最新版本的代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git checkout v0.5.1&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;

&lt;li&gt;
&lt;p&gt;现在可以编译当前最新版本v0.5.1代码了(windows平台忽略，暂时只关心Linux平台)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sh build.sh&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;build.sh脚本是Heka的编译工具，整个工程是通过cmake来管理的。第一次build过程可能比较慢，因为还会下载一些其他的依赖库和工具，不过不需要人为干预，坐等build完成即可。build完成后，当前源码目录下多出一个build目录： &lt;div&gt;
&lt;img height='280' src='/assets/images/heka-build.png' width='450' /&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;build目录中的heka目录是我们需要关注的。这里放置了所有编译结果，包括heka可执行的二进制文件等。 &lt;div&gt;
&lt;img height='200' src='/assets/images/heka-build-bin.png' width='500' /&gt;
&lt;/div&gt; &lt;strong&gt;hekad&lt;/strong&gt;文件就是我们最关心的二进制执行文件。只需要这个二进制加上配置文件就可以运行整个Heka软件。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;br /&gt;
&lt;h4 id='heka'&gt;配置Heka实例&lt;/h4&gt;

&lt;p&gt;为了能够直观的感受Heka，我们配置一个简单的实例，让它监控本机上的nginx access日志目录，实时的读取增量日志，并做条数统计，然后将结果打印到屏幕。&lt;/p&gt;

&lt;p&gt;Nginx Access日志的目录路径是：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/Users/marckywu/projects/logserver2/tmp/logs&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Nginx Access日志文件名(不做rotation)是：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;access.log&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Nginx Access日志format是：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;#39;$remote_addr - [$time_local] &amp;quot;$request&amp;quot; $status $body_bytes_sent &amp;quot;$http_referer&amp;quot; &amp;quot;$http_user_agent&amp;quot; &amp;quot;$http_x_forwarded_for&amp;quot;&amp;#39;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;基于这三个必要的信息，Heka配置文件如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[hekad]
base_dir = &amp;quot;/tmp/hekad/cache&amp;quot;
share_dir = &amp;quot;/Users/marckywu/github/heka/build/heka&amp;quot;


[LogstreamerInput]
log_directory = &amp;quot;/Users/marckywu/projects/logserver2/tmp/logs&amp;quot;
file_match = &amp;#39;access\.log&amp;#39;
decoder = &amp;quot;FxaNginxAccessDecoder&amp;quot;


[FxaNginxAccessDecoder]
type = &amp;quot;SandboxDecoder&amp;quot;
script_type = &amp;quot;lua&amp;quot;
filename = &amp;quot;/Users/marckywu/github/heka/sandbox/lua/decoders/nginx_access.lua&amp;quot;
module_directory = &amp;quot;modules&amp;quot;
    [FxaNginxAccessDecoder.config]
    log_format = &amp;#39;$remote_addr - [$time_local] &amp;quot;$request&amp;quot; $status $body_bytes_sent &amp;quot;$http_referer&amp;quot; &amp;quot;$http_user_agent&amp;quot; &amp;quot;$http_x_forwarded_for&amp;quot;&amp;#39;
    type = &amp;quot;nginx&amp;quot;


[CounterFilter]
message_matcher = &amp;quot;Type == &amp;#39;nginx&amp;#39;&amp;quot;

[print]
type = &amp;quot;LogOutput&amp;quot;
message_matcher = &amp;quot;Type == &amp;#39;heka.counter-output&amp;#39;&amp;quot;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将配置文件保存为hekad.toml，注意heka的配置文件是toml语法。我们启动hekad进程：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/hekad -config hekad.toml&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个时候，Heka已经开始监控nginx access日志了，只要有日志数据，就会读取并处理。我们用ab发送100个请求给nginx，产生100条日志看看Heka的打印效果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2014/04/30 13:54:36 &amp;lt;
	Timestamp: 2014-04-30 13:54:36.136374705 +0800 CST
	Type: heka.counter-output
	Hostname: WudeMacBook-Pro.local
	Pid: 56972
	UUID: 0ed1f5c9-5691-45f9-9c4d-a044b70f17ad
	Logger:
	Payload: Got 100 messages. 20.00 msg/sec
	EnvVersion:
	Severity: 7
	Fields: []
&amp;gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面打印到屏幕中的&lt;strong&gt;Payload: Got 100 messages. 20.00 msg/sec&lt;/strong&gt;，就是counter插件的统计计算结果，counter插件是Heka自带的一个filter插件，这里打印到屏幕也是用的Heka自带的LogOut插件。&lt;/p&gt;
&lt;br /&gt;
&lt;h4 id='id20'&gt;插件编译&lt;/h4&gt;

&lt;p&gt;在我们克隆出来的Heka源码目录中有一个examples目录，里面有几个插件开发的示例，我们选取host_filter.go插件来试图编译一次。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;在Heka的源码编译目录创建放置插件代码的目录&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p externals/example&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;

&lt;li&gt;
&lt;p&gt;将host_filter.go拷贝到刚创建的目录中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp examples/host_filter.go externals/example/&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;

&lt;li&gt;
&lt;p&gt;在cmake目录创建plugin_loader.cmake文件，内容：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;add_external_plugin(svn http://xx.taobao.com/trunk/example :local)&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意：svn路径的最后一个目录名字必须与第一步创建的目录相同；&lt;strong&gt;:local&lt;/strong&gt;标志就是代表从第一步创建的externals目录获取源码，否则就会自动的从此svn地址checkout源码来编译，所以插件开发阶段此处应该是&lt;strong&gt;:local&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;

&lt;li&gt;
&lt;p&gt;最后重新编译Heka&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sh build.sh&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在build出来的hekad二进制文件就已经包含了新增加的插件了。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;br /&gt;
&lt;h4 id='id21'&gt;插件开发&lt;/h4&gt;

&lt;p&gt;Heka可以采用Go或者Lua开发插件，本文只介绍Go语言开发插件。具体业务数据计算需求基本都是通过开发Filter插件来完成，介绍一个Filter插件的大体框架：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type DemoFilter struct {

}

func (f *DemoFilter) Init(config interface{}) error {
	return nil
}

func (f *DemoFilter) Run(runner pipeline.FilterRunner, helper pipeline.PluginHelper) (
	err error) {
	
	for pack := range runner.InChan() {
	
		pack.Recycle()	
	}
	
	
	return
}

func init() {
	pipeline.RegisterPlugin(&amp;quot;DemoFilter&amp;quot;, func() interface{} {
		return new(DemoFilter)
	})
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;开发一个Filter插件，只需要定义一个插件对象，然后将对象通过init函数注册上插件即可。此处我们将filter插件对象定义为&lt;code&gt;DemoFilter&lt;/code&gt;，它同时需要实现&lt;code&gt;Init&lt;/code&gt;和&lt;code&gt;Run&lt;/code&gt;两个方法，Init方法主要是获取配置文件设置的配置选项；Run方法是监听自己的输入channel，接收消息，然后进行处理。&lt;/p&gt;
&lt;br /&gt;
&lt;pre&gt;&lt;code&gt;pipeline.RegisterPlugin(&amp;quot;DemoFilter&amp;quot;, func() interface{} {
	return new(DemoFilter)
})&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#8220;DemoFilter&amp;#8221;字符串是插件的名字或者也可以当做类型。这个将在配置文件使用。&lt;/p&gt;
&lt;br /&gt;
&lt;pre&gt;&lt;code&gt;for pack := range runner.InChan() {
	
	pack.Recycle()	
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;runner.InChan()调用其实是返回的插件的输入channel，也就是数据将从这里流入到这个插件，pack就是获取到的一个消息，消息类型是：&lt;code&gt;*PipelinePack&lt;/code&gt;，所有进入Heka的数据都被封装成了PipelinePack在内部各个组件之间传输，这是插件开发将直接打交道的最重要的一个对象。当我们把一个pack处理完后，不再需要将pack传递给下一个组件时，也就是这个pack的生命结束，那么我们需要释放它，于是调用pack.Recycle()方法。Recycle的目的是缓存pack对象，留给下一个数据使用，可以降低gc压力。&lt;/p&gt;
&lt;br /&gt;
&lt;p&gt;Filter插件的开发，可以学习examples/host_filter.go。Heka本身就自带了很多的插件，都可以作为学习的目标。我觉得要深刻的理解插件开发，还是需要熟悉heka核心源码才行。&lt;/p&gt;
&lt;br /&gt;
&lt;p&gt;玩开心。。。&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>深入Go语言网络库的基础实现</title>
   <link href="http://skoo87.github.io/go/2014/04/21/go-net-core"/>
   <updated>2014-04-21T00:00:00+08:00</updated>
   <id>http://skoo87.github.io/go/2014/04/21/go-net-core</id>
   <content type="html">&lt;p&gt;Go语言的出现，让我见到了一门语言把网络编程这件事情给做“正确”了，当然，除了Go语言以外，还有很多语言也把这件事情做&amp;#8221;正确&amp;#8221;了。我一直坚持着这样的理念——&lt;font color='red'&gt;要做&quot;正确&quot;的事情，而不是&quot;高性能&quot;的事情&lt;/font&gt;；很多时候，我们在做系统设计、技术选型的时候，都被“高性能”这三个字给绑架了，当然不是说性能不重要，你懂的。&lt;/p&gt;

&lt;p&gt;目前很多高性能的基础网络服务器都是采用的C语言开发的，比如：Nginx、Redis、memcached等，它们都是基于&amp;#8221;事件驱动 + 事件回掉函数&amp;#8221;的方式实现，也就是采用epoll等作为网络收发数据包的核心驱动。不少人(包括我自己)都认为“事件驱动 + 事件回掉函数”的编程方法是“反人类”的；因为大多数人都更习惯线性的处理一件事情，做完第一件事情再做第二件事情，并不习惯在N件事情之间频繁的切换干活。为了解决程序员在开发服务器时需要自己的大脑不断的“上下文切换”的问题，Go语言引入了一种用户态线程goroutine来取代编写异步的事件回掉函数，从而重新回归到多线程并发模型的线性、同步的编程方式上。&lt;/p&gt;

&lt;p&gt;用Go语言写一个最简单的echo服务器：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
	&amp;quot;log&amp;quot;
	&amp;quot;net&amp;quot;
)

func main() {
	ln, err := net.Listen(&amp;quot;tcp&amp;quot;, &amp;quot;:8080&amp;quot;)
	if err != nil {
        	log.Println(err)
        	return
	}
	for {
        	conn, err := ln.Accept()
        	if err != nil {
            	log.Println(err)
            	continue
        	}

        	go echoFunc(conn)
	}
}

func echoFunc(c net.Conn) {
	buf := make([]byte, 1024)

	for {
        	n, err := c.Read(buf)
        	if err != nil {
            	log.Println(err)
            	return
        	}

        	c.Write(buf[:n])
	}
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;main函数的过程就是首先创建一个监听套接字，然后用一个for循环不断的从监听套接字上Accept新的连接，最后调用echoFunc函数在建立的连接上干活。关键代码是:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go echoFunc(conn)&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每收到一个新的连接，就创建一个“线程”去服务这个连接，因此所有的业务逻辑都可以同步、顺序的编写到echoFunc函数中，再也不用去关心网络IO是否会阻塞的问题。不管业务多复杂，Go语言的并发服务器的编程模型都是长这个样子。可以肯定的是，在linux上Go语言写的网络服务器也是采用的epoll作为最底层的数据收发驱动，Go语言网络的底层实现中同样存在“上下文切换”的工作，只是这个切换工作由runtime的调度器来做了，减少了程序员的负担。&lt;/p&gt;

&lt;p&gt;弄明白网络库的底层实现，貌似只要弄清楚echo服务器中的Listen、Accept、Read、Write四个函数的底层实现关系就可以了。本文将采用自底向上的方式来介绍，也就是从最底层到上层的方式，这也是我阅读源码的方式。底层实现涉及到的核心源码文件主要有：&lt;br /&gt; &lt;a href='http://golang.org/src/pkg/net/fd_unix.go'&gt;net/fd_unix.go&lt;/a&gt; &lt;br /&gt; &lt;a href='http://golang.org/src/pkg/net/fd_poll_runtime.go'&gt;net/fd_poll_runtime.go&lt;/a&gt;&lt;br /&gt; &lt;a href='http://golang.org/src/pkg/runtime/netpoll.goc'&gt;runtime/netpoll.goc&lt;/a&gt; &lt;br /&gt; &lt;a href='http://golang.org/src/pkg/runtime/netpoll_epoll.c'&gt;runtime/netpoll_epoll.c&lt;/a&gt; &lt;br /&gt; &lt;a href='http://golang.org/src/pkg/runtime/proc.c'&gt;runtime/proc.c&lt;/a&gt; (调度器)&lt;/p&gt;

&lt;p&gt;netpoll_epoll.c文件是Linux平台使用epoll作为网络IO多路复用的实现代码，这份代码可以了解到epoll相关的操作（比如：添加fd到epoll、从epoll删除fd等），只有4个函数，分别是runtime·netpollinit、runtime·netpollopen、runtime·netpollclose和runtime·netpoll。init函数就是创建epoll对象，open函数就是添加一个fd到epoll中，close函数就是从epoll删除一个fd，netpoll函数就是从epoll wait得到所有发生事件的fd，并将每个fd对应的goroutine(用户态线程)通过链表返回。用epoll写过程序的人应该都能理解这份代码，没什么特别之处。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void
runtime·netpollinit(void)
{
	epfd = runtime·epollcreate1(EPOLL_CLOEXEC);
	if(epfd &amp;gt;= 0)
		return;
	epfd = runtime·epollcreate(1024);
	if(epfd &amp;gt;= 0) {
		runtime·closeonexec(epfd);
		return;
	}
	runtime·printf(&amp;quot;netpollinit: failed to create descriptor (%d)\n&amp;quot;, -epfd);
	runtime·throw(&amp;quot;netpollinit: failed to create descriptor&amp;quot;);
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;runtime·netpollinit函数首先使用runtime·epollcreate1创建epoll实例，如果没有创建成功，就换用runtime·epollcreate再创建一次。这两个create函数分别等价于glibc的epoll_create1和epoll_create函数。只是因为Go语言并没有直接使用glibc，而是自己封装的系统调用，但功能是等价于glibc的。可以通过man手册查看这两个create的详细信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int32
runtime·netpollopen(uintptr fd, PollDesc *pd)
{
	EpollEvent ev;
	int32 res;
	
	ev.events = EPOLLIN|EPOLLOUT|EPOLLRDHUP|EPOLLET;
	ev.data = (uint64)pd;
	res = runtime·epollctl(epfd, EPOLL_CTL_ADD, (int32)fd, &amp;amp;ev);
	return -res;
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;添加fd到epoll中的runtime·netpollopen函数可以看到每个fd一开始都关注了读写事件，并且采用的是边缘触发，除此之外还关注了一个不常见的新事件EPOLLRDHUP，这个事件是在较新的内核版本添加的，目的是解决对端socket关闭，epoll本身并不能直接感知到这个关闭动作的问题。注意任何一个fd在添加到epoll中的时候就关注了EPOLLOUT事件的话，就立马产生一次写事件，这次事件可能是多余浪费的。&lt;/p&gt;

&lt;p&gt;epoll操作的相关函数都会在事件驱动的抽象层中去调用，为什么需要这个抽象层呢？原因很简单，因为Go语言需要跑在不同的平台上，有Linux、Unix、Mac OS X和Windows等，所以需要靠事件驱动的抽象层来为网络库提供一致的接口，从而屏蔽事件驱动的具体平台依赖实现。runtime/netpoll.goc源文件就是整个事件驱动抽象层的实现，抽象层的核心数据结构是：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;struct PollDesc
{
	PollDesc* link;	// in pollcache, protected by pollcache.Lock
	Lock;		// protectes the following fields
	uintptr	fd;
	bool	closing;
	uintptr	seq;	// protects from stale timers and ready notifications
	G*	rg;	// G waiting for read or READY (binary semaphore)
	Timer	rt;	// read deadline timer (set if rt.fv != nil)
	int64	rd;	// read deadline
	G*	wg;	// the same for writes
	Timer	wt;
	int64	wd;
};&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每个添加到epoll中的fd都对应了一个PollDesc结构实例，PollDesc维护了读写此fd的goroutine这一非常重要的信息。可以大胆的推测一下，网络IO读写操作的实现应该是：当在一个fd上读写遇到EAGAIN错误的时候，就将当前goroutine存储到这个fd对应的PollDesc中，同时将goroutine给park住，直到这个fd上再此发生了读写事件后，再将此goroutine给ready激活重新运行。事实上的实现大概也是这个样子的。&lt;/p&gt;

&lt;p&gt;事件驱动抽象层主要干的事情就是将具体的事件驱动实现（比如： epoll）通过统一的接口封装成Go接口供net库使用，主要的接口也是：&lt;em&gt;创建事件驱动实例&lt;/em&gt;、&lt;em&gt;添加fd&lt;/em&gt;、&lt;em&gt;删除fd&lt;/em&gt;、&lt;em&gt;等待事件&lt;/em&gt;以及&lt;em&gt;设置DeadLine&lt;/em&gt;。&lt;code&gt;runtime_pollServerInit&lt;/code&gt;负责创建事件驱动实例，&lt;code&gt;runtime_pollOpen&lt;/code&gt;将分配一个PollDesc实例和fd绑定起来，然后将fd添加到epoll中，&lt;code&gt;runtime_pollClose&lt;/code&gt;就是将fd从epoll中删除，同时将删除的fd绑定的PollDesc实例删除，&lt;code&gt;runtime_pollWait&lt;/code&gt;接口是至关重要的，这个接口一般是在非阻塞读写发生EAGAIN错误的时候调用，作用就是park当前读写的goroutine。&lt;/p&gt;

&lt;p&gt;runtime中的epoll事件驱动抽象层其实在进入net库后，又被封装了一次，这一次封装从代码上看主要是为了方便在纯Go语言环境进行操作，net库中的这次封装实现在net/fd_poll_runtime.go文件中，主要是通过pollDesc对象来实现的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type pollDesc struct {
	runtimeCtx uintptr
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意：此处的pollDesc对象不是上文提到的runtime中的PollDesc，相反此处pollDesc对象的runtimeCtx成员才是指向的runtime的PollDesc实例。pollDesc对象主要就是将runtime的事件驱动抽象层给再封装了一次，供网络fd对象使用。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var serverInit sync.Once

func (pd *pollDesc) Init(fd *netFD) error {
	serverInit.Do(runtime_pollServerInit)
	ctx, errno := runtime_pollOpen(uintptr(fd.sysfd))
	if errno != 0 {
		return syscall.Errno(errno)
	}
	pd.runtimeCtx = ctx
	return nil
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;pollDesc对象最需要关注的就是其Init方法，这个方法通过一个sync.Once变量来调用了runtime_pollServerInit函数，也就是创建epoll实例的函数。意思就是runtime_pollServerInit函数在整个进程生命周期内只会被调用一次，也就是只会创建一次epoll实例。epoll实例被创建后，会调用runtime_pollOpen函数将fd添加到epoll中。&lt;/p&gt;

&lt;p&gt;网络编程中的所有socket fd都是通过netFD对象实现的，netFD是对网络IO操作的抽象，linux的实现在文件net/fd_unix.go中。netFD对象实现有自己的init方法，还有完成基本IO操作的Read和Write方法，当然除了这三个方法以外，还有很多非常有用的方法供用户使用。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Network file descriptor.
type netFD struct {
	// locking/lifetime of sysfd + serialize access to Read and Write methods
	fdmu fdMutex

	// immutable until Close
	sysfd       int
	family      int
	sotype      int
	isConnected bool
	net         string
	laddr       Addr
	raddr       Addr

	// wait server
	pd pollDesc
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过netFD对象的定义可以看到每个fd都关联了一个pollDesc实例，通过上文我们知道pollDesc对象最终是对epoll的封装。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (fd *netFD) init() error {
	if err := fd.pd.Init(fd); err != nil {
		return err
	}
	return nil
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;netFD对象的init函数仅仅是调用了pollDesc实例的Init函数，作用就是将fd添加到epoll中，如果这个fd是第一个网络socket fd的话，这一次init还会担任创建epoll实例的任务。要知道在Go进程里，只会有一个epoll实例来管理所有的网络socket fd，这个epoll实例也就是在第一个网络socket fd被创建的时候所创建。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for {
	n, err = syscall.Read(int(fd.sysfd), p)
	if err != nil {
		n = 0
		if err == syscall.EAGAIN {
			if err = fd.pd.WaitRead(); err == nil {
				continue
			}
		}
	}
	err = chkReadErr(n, err, fd)
	break
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码段是从netFD的Read方法中摘取，重点关注这个for循环中的syscall.Read调用的错误处理。当有错误发生的时候，会检查这个错误是否是syscall.EAGAIN，如果是，则调用WaitRead将当前读这个fd的goroutine给park住，直到这个fd上的读事件再次发生为止。当这个socket上有新数据到来的时候，WaitRead调用返回，继续for循环的执行。这样的实现，就让调用netFD的Read的地方变成了同步“阻塞”方式编程，不再是异步非阻塞的编程方式了。netFD的Write方法和Read的实现原理是一样的，都是在碰到EAGAIN错误的时候将当前goroutine给park住直到socket再次可写为止。&lt;/p&gt;

&lt;p&gt;本文只是将网络库的底层实现给大体上引导了一遍，知道底层代码大概实现在什么地方，方便结合源码深入理解。Go语言中的高并发、同步阻塞方式编程的关键其实是”goroutine和调度器”，针对网络IO的时候，我们需要知道EAGAIN这个非常关键的调度点，掌握了这个调度点，即使没有调度器，自己也可以在epoll的基础上配合协程等用户态线程实现网络IO操作的调度，达到同步阻塞编程的目的。&lt;/p&gt;

&lt;p&gt;最后，为什么需要同步阻塞的方式编程？只有看多、写多了异步非阻塞代码的时候才能够深切体会到这个问题。真正的高大上绝对不是——“别人不会，我会；别人写不出来，我写得出来。”&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>Heka, 一个高可扩展的实时数据收集和处理工具</title>
   <link href="http://skoo87.github.io/system/2014/04/02/hekad"/>
   <updated>2014-04-02T00:00:00+08:00</updated>
   <id>http://skoo87.github.io/system/2014/04/02/hekad</id>
   <content type="html">&lt;p&gt;一年以前我们在着手为阿里集团内部实现一个日志运维平台的时候，mozilla开源了Heka这个工具，当时由于我们的理念和Heka不太一致，也就没有过多的参考它、甚至是使用它。（当然，我们的日志运维平台最终并没有做太成功；现在来看，当初没借鉴Heka是一个不太正确的选择）。今天我在从事CDN调度系统开发的过程中，发现我们的一个子系统以及一些工具好像挺适合用Heka来做。于是我再次深入研究了一下Heka和阅读了其5，6K行的核心源码。&lt;/p&gt;

&lt;p&gt;&lt;a href='https://github.com/mozilla-services/heka'&gt;Heka&lt;/a&gt; 是一个高可扩展的数据收集和处理工具。它的可扩展性不仅仅是体现在程序本身可以进行插件开发、更可以方便的通过添加机器进行水平扩展。Heka是一个使用Go语言开发的工具，大量使用了Go的goroutine并发和channel通信，通过我们做日志平台的经验来看，一般情况下性能问题不需要太多的顾虑。&lt;/p&gt;

&lt;p&gt;Heka程序的可扩展性体现在它的插件开发上。Heka核心是Go语言开发，其插件理所当然的可以采用Go语言开发，当然你还可以通过lua来开发插件，如果你更喜欢写lua的话；lua开发的插件具备热更新的能力，也就是修改了lua插件的代码，并不需要重启Heka进程，在某些应用场景下，热更新具备很强的竞争力。根据实际的应用需求，可以为Heka开发4种类型的插件，这4种插件就构成了Heka的整体结构和功能，所以Heka的核心代码量（非插件）只有5，6K行。可开发的4种插件分别是：Input、Decoder、Filter、Output。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Input插件&lt;/strong&gt;就是Heka的数据输入源。假设需要从一个日志文件读入数据，那就只需要开发一个从日志文件读取日志数据的Input插件即可。值得开心的事情是，Heka已经默认自带了多个Input插件，可以满足大部分数据来源的需求。自带的&lt;code&gt;LogstreamerInput&lt;/code&gt;插件就是用来从日志文件实时读入数据用的，它支持读取各种rotate规则的日志文件路径；&lt;code&gt;HttpInput&lt;/code&gt;插件可以间断式的通过访问一个URL地址来获取数据，比如：Web服务器的7层健康检查场景就可以使用HttpInput插件来探测。&lt;code&gt;HttpListenInput&lt;/code&gt;插件会启动一个Http服务器接收外部访问来获取数据，比如：通过curl等命令直接将数据post到Heka；还有两个比较重要的Input插件：分别是&lt;code&gt;TcpInput&lt;/code&gt;和&lt;code&gt;UdpInput&lt;/code&gt;，有这两个插件外部程序就可以通过TCP/UDP将数据发送给Heka。&lt;a href='http://hekad.readthedocs.org/en/latest/config/inputs/index.html'&gt;http://hekad.readthedocs.org/en/latest/config/inputs/index.html&lt;/a&gt;, 这里可以看到Heka提供的全部Input插件和详细使用方法，大部分情况都不需要自己开发Input插件了。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Decoder插件&lt;/strong&gt;，各种Input插件负责将原始数据送入到Heka内部，这些数据一般来说都是具备一定的格式，比如：Nginx access日志、Syslog协议数据、自定义的数据格式等等，Decoder插件干的事情就是将Input插件输入的一个个的原始数据消息给解析一遍，最终得到一个结构化好的消息，不再是一个非结构化的原始数据消息。结构化的消息更利于编程进行处理。&lt;a href='http://hekad.readthedocs.org/en/latest/config/decoders/index.html'&gt;http://hekad.readthedocs.org/en/latest/config/decoders/index.html&lt;/a&gt;，这里例举了Heka自带的所有Decoder插件，我最关注的插件是：&lt;code&gt;Nginx Access Log Decoder&lt;/code&gt;和 &lt;code&gt;Syslog Decoder&lt;/code&gt;，这两个插件都是Lua开发的。&lt;code&gt;注意，每个Input插件得有一个Decoder插件负责对输入的数据进行解析到结构化的程度&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Filter插件&lt;/strong&gt;干的事情就是负责具体的数据分析、计算任务。Heka默认也带了好几个Filter插件，但都不是我的菜，绝大多数时候，可能都需要我们自己根据应用需求开发自己的Filter插件来完成数据分析、计算工作。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;OutPut插件&lt;/strong&gt;负责将Heka内部的一个个消息输出到外部环境，比如：文件、数据库、消息队列等；注意，也可以通过TcpOutput将消息输出到下一个Heka继续处理，这样就可以部署成多机分布式结构，只要有必要。&lt;/p&gt;

&lt;p&gt;通过4种类型的插件，基本可以了解到Heka是基于Pipeline方式对数据进行实时处理。除了可以开发4种插件以外，Heka还提供了一个很高端的机制——&lt;code&gt;message matcher&lt;/code&gt;，message matcher是应用在Filter和Output两种插件身上，它主要是用来指定哪些消息由哪些插件(Filter/Output)处理。有了message matcher机制就可以通过配置文件实现不同的数据由不同的Filter进行计算、不同的Output输出到不同的外部环境。没有message_matcher，Heka的价值就会大打折扣了。&lt;/p&gt;

&lt;h5 id='hekas_agentaggregator'&gt;Heka&amp;#8217;s Agent/Aggregator架构&lt;/h5&gt;
&lt;div align='center'&gt;
&lt;img height='250' src='/assets/images/heka.png' width='300' /&gt;
&lt;/div&gt;
&lt;p&gt;Heka可以通过配置文件部署成为不同的角色，实际上它们都是同一个二进制程序。上图中的圆形组件Heka担任的是Agent角色，而矩形组件Heka担任的是Aggregator角色。假设每个Agent部署在不同的主机上，使用LogstreamerInput插件负责监控、采集Nginx Access日志，然后将日志数据通过Nginx Access Decoder插件进行解析，最后通过特定的Filter插件做一些分析、计算工作，最终的计算结果再通过TcpOutput插件发送到扮演Aggregator角色的Heka进行聚合、汇总计算从而得到所有主机的日志计算结果。Heka具备这样的一个扩展架构，可以非常方便的将计算任务分摊到多机，从而实现类MapReduce，当然Heka仅仅只是一个轻量级的小工具，不是一个分布式计算平台。&lt;/p&gt;

&lt;h5 id='hekas_agentrouter'&gt;Heka&amp;#8217;s Agent/Router架构&lt;/h5&gt;
&lt;div align='center'&gt;
&lt;img height='250' src='/assets/images/heka-router.png' width='300' /&gt;
&lt;/div&gt;
&lt;p&gt;除了Agent/Aggregator架构外，还可以把Heka当做一个Router来使用，图中圆形组件Heka还是Agent，每个Agent负责采集不同的数据发送给矩形组件Heka，也就是Router。Heka Router可以通过message matcher机制将不同的数据输出到不同的外部存储等，从而实现一个Router的功能。当然，上面的两种架构也是可以混合到一起使用的，Heka的系统级扩展性还是足够灵活的。Agent/Router架构其实非常像淘宝开源的DataX工具(DataX是由淘宝 @泽远 同学领导开发的一款各种存储间数据交换的瑞士军刀。DataX也是支持各种插件开发，可以通过插件实现MySql数据同步到Oracle、Oracle同步到MySql、Mysql同步HDFS等等)，在没有DataX以前，就是各种存储间同步的独立工具，由不同的人开发，使用方式也完全不一致，零散的小工具极其不友好。在一个灵活、可扩展的工具上面，进行扩展开发实现各种业务需求，可以让系统的运维更加的友好，部署、使用方式也更加的一致。&lt;/p&gt;

&lt;p&gt;Heka能够自定义Input/Output插件，再配合message matcher机制，我认为这才是最具想象力的部分。输入输出是直接和外部交互，有了扩展外部交互的能力，我就可以将一切零散的小工具（各种脚本等）整合起来，统一维护，从而实现出一整套完整的应用逻辑；就好比Nginx一样，各种Http相关需求都可以通过开发模块来实现，最后将多个模块组合起来就构成一个完整的应用解决方案。&lt;/p&gt;

&lt;p&gt;最后，还是轻量，足够轻量运维成本才足够低。&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>inet_aton的坑</title>
   <link href="http://skoo87.github.io/system/2014/03/19/inet-aton"/>
   <updated>2014-03-19T00:00:00+08:00</updated>
   <id>http://skoo87.github.io/system/2014/03/19/inet-aton</id>
   <content type="html">&lt;p&gt;最近小踩了一下inet_aton的坑。我们把ip数据存储在mysql中，ip的存储不是采用的点分十进制，而是使用inet_aton转换成整数后再存储的；我们用C写的服务器会用一个ip的整数值去查询mysql，查询的结果死活对不上。最后发现C程序转换ip得到的整数和mysql不一样，字节序刚好是相反的。这个时候其实我基本知道是ip转换的时候采用的字节序不一样，一个是网络序，另一个是主机序。在linux上man inet_aton可以看到：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;inet_aton() converts the Internet host address cp from the IPv4 numbers-and-dots notation into binary form (in network byte order) and stores it in the structure that inp points to.&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明glibc的inet_aton得到的结果是网络序，不是主机序的。然后，再查一下mysql关于&lt;a href='https://dev.mysql.com/doc/refman/5.7/en/miscellaneous-functions.html#function_inet-aton'&gt;inet_aton&lt;/a&gt;的文档:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Given the dotted-quad representation of an IPv4 network address as a string, returns an integer that represents the numeric value of the address in network byte order (big endian).

mysql&amp;gt; SELECT INET_ATON(&amp;#39;10.0.5.9&amp;#39;);
    -&amp;gt; 167773449
For this example, the return value is calculated as 10×256^3 + 0×256^2 + 5×256 + 9.&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我X，mysql的文档说它的inet_aton的结果也是网络序呢。但它举了一例子，这个例子给出的计算方法却是一个主机序的结果哦。事实上，现在mysql的inet_aton得到的结果确实是主机序，根本不是网络序。不知道是文档bug，还是实现bug了。&lt;/p&gt;

&lt;p&gt;反正我们需要注意：glibc的inet_aton和mysql的inet_aton的字节许是不同的，在现在的小端机上就是刚好相反。解决方法应该是，将我们程序中的网络序的ip整数通过ntohl函数转换成主机序后，再去查mysql。&lt;/p&gt;

&lt;p&gt;最后贴一个Nginx点分十进制ip转整数的实现：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;in_addr_t
ngx_inet_addr(u_char *text, size_t len)
{
	u_char      *p, c;
	in_addr_t    addr;
	ngx_uint_t   octet, n;

	addr = 0;
	octet = 0;
	n = 0;

	for (p = text; p &amp;lt; text + len; p++) {

    	c = *p;

    	if (c &amp;gt;= &amp;#39;0&amp;#39; &amp;amp;&amp;amp; c &amp;lt;= &amp;#39;9&amp;#39;) {
        	octet = octet * 10 + (c - &amp;#39;0&amp;#39;);
        	continue;
    	}

    	if (c == &amp;#39;.&amp;#39; &amp;amp;&amp;amp; octet &amp;lt; 256) {
        	addr = (addr &amp;lt;&amp;lt; 8) + octet;
        	octet = 0;
        	n++;
        	continue;
    	}

    	return INADDR_NONE;
	}

	if (n != 3) {
    	return INADDR_NONE;
	}

	if (octet &amp;lt; 256) {
    	addr = (addr &amp;lt;&amp;lt; 8) + octet;
    	return htonl(addr);
	}

	return INADDR_NONE;
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意：return htonl(addr)，说明最后的结果是是转换成了网络序，和glibc的inet_aton行为是相符的。&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>使用reuseport和recvmmsg优化UDP服务器</title>
   <link href="http://skoo87.github.io/system/2014/03/18/udp-server-performance"/>
   <updated>2014-03-18T00:00:00+08:00</updated>
   <id>http://skoo87.github.io/system/2014/03/18/udp-server-performance</id>
   <content type="html">&lt;p&gt;最近刚好完成了一个DNS服务器的开发，因此积累一点对高性能UDP服务器的开发经验。如果你也遇到UDP服务器的性能不佳，远不如你的预期，也许你也可以采用本文的手段去优化一下试试。&lt;/p&gt;

&lt;p&gt;udp不像tcp是有连接的，因此udp不能通过建立多个连接来提高对服务器的并发访问，然后我就遇到了在多核环境下通过多线程访问一个共享的udp socket时，无论如何我都无法将所有的cpu都利用起来，最后的结局当然就是无法压测出机器的瓶颈，性能也上不去。Google为了解决他们的DNS服务器性能问题，就给linux内核打了一个patch，这个patch就是SO_REUSEPORT，经过我的实战体验，reuseport对udp服务器在多核机器上的性能提升是非常大的，值得使用。&lt;/p&gt;

&lt;p&gt;&lt;a href='https://lwn.net/Articles/542629/'&gt;REUSEPORT&lt;/a&gt; 的目的如其名，就是为了让多线程/多进程服务器的每个线程都listen同一个端口，并且最终每个线程拥有一个独立的socket，而不是所有线程都访问一个socket。没有reuseport这个patch的话，这么做的后果就是服务器会报出一个类似“地址/端口被占用的”错误信息。在没有reuseport的时候，客户端发给udp服务器的每个包都是被投递到唯一的一个socket上了，使用reuseport后，服务器有了多个socket，那么客户端发过来的包投递到哪个socket上呢？linux内核采用了一个四元组&amp;lt;客户端ip，客户端port，服务器ip，服务器port&amp;gt;的hash来进行包的分发，这样做至少有两个目的：一是保证同一个客户度过来的包都被递送到同一个socket上；二是在客户端量足够的时候，基本可以均衡到所有的socket上。在使用reuseport的时候需要注意：客户端太少的话，是很难压测出服务器的真实性能的，因为reuseport使用的是hash值来分发请求到socket上，所以可能出现每个socket上接收包不均衡的情况，使用较多的客户端机器来压测服务器，目的就是让每个socket尽可能的均衡。&lt;/p&gt;

&lt;p&gt;使用reuseport后，udp服务器的并发能力大幅度的提高了，这个时候还可以继续使用&lt;a href='http://man7.org/linux/man-pages/man2/recvmmsg.2.html'&gt;recvmmsg&lt;/a&gt;来继续降低系统调用的开销。recvmmsg是一个批量接口，它可以从socket里一次读出多个udp数据包，不像recvfrom那样一次只能读一个。如果客户端多、请求量大的话，recvmmsg的批量读就很有优势了。不过，使用recvmmsg一定要清楚，它从socket里一次读出的所有包不一定是来自同一个客户端的，大多数情况应该都是来自不同客户端的。这不像tcp，从同一个连接里读到的数据一定是同一个客户端。我们的一个同学在使用recvmmsg的时候，就犯了这个错误，误认为一次收取的数据包都是同一个客户端的，最后将所有的应答都发给了同一个客户端，其他的客户端全都超时了。高性能服务器开发中，系统调用是昂贵的，所以没事就可以用strace看看一个请求周期内有哪些系统调用，尽一切可能去优化掉他们。&lt;/p&gt;

&lt;p&gt;开发一个UDP服务器，不是说使用了reuseport和recvmmsg后性能就高了。一个高性能的网络服务器，是需要进行方方面面的优化才行的。&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>调度开发中的那些事</title>
   <link href="http://skoo87.github.io/system/2014/03/09/cdn-scheduler-conclusion"/>
   <updated>2014-03-09T00:00:00+08:00</updated>
   <id>http://skoo87.github.io/system/2014/03/09/cdn-scheduler-conclusion</id>
   <content type="html">&lt;p&gt;最近两个月一直在做阿里CDN的DNS调度器重构开发，其实就是重写，谈不上是重构，所以到头来还是没有积累上正确重构的经验。&lt;/p&gt;

&lt;p&gt;DNS调度器的开发让我第一次深入研究了DNS服务器以及DNS协议内容，DNS协议属于二进制协议，设计得比较简单，但也有一些明显的不足之处，所以DNS才搞出了opt additional rr进行协议上的扩展以期满足更加丰富的需求。RFC规定了DNS请求可以走UDP，也可以走TCP，所以一个DNS服务器应该同时支持这两种传输层；由于TCP服务器在实现的时候需要严格考虑TCP的字节流传输机制，所以在编程上面比UDP复杂很多，DNS规范为了让DNS的TCP服务器开发更加的简单，于是就给协议数据包加上了2个字节的长度头部，有了长度头就可以严格检测服务器是否收全了一个完整的DNS请求，只有在收全的时候才开始做DNS协议解析，从而避免流式的解析协议。其实，对于DNS来说，这两个字节的长度头是完全多余的，没有长度头通过流式的解析协议也可以正确无误的解析出一个DNS请求，只是对编程的要求更高了而已。这次自己对DNS协议实现了一遍，更加的让自己对协议设计有了深刻的认识，一个好的协议首先是&lt;code&gt;满足当前的业务需求&lt;/code&gt;；其次&lt;code&gt;协议是易于扩展的&lt;/code&gt;（对于扩展性这一条来说，DNS设计得并不算好，opt additional rr在语意上来说真是一个很鸡肋的设计，但除此之外又没别的办法了。），&lt;code&gt;协议应该是高效的&lt;/code&gt;（比如：HTTP可以全流式解析；DNS进行了域名压缩，可以减少传输的数据量；SPDY进行了连接复用，支持并发请求等等），高效的协议也许我们还应该关注一点的就是可缓存，缓存就是一张“狗皮膏药”，哪里不行贴哪里，哈哈。最后，&lt;code&gt;协议设计非常重要的一点就是易于实现&lt;/code&gt;，一个好的协议应该实现是简单的，大多数情况简单就代表不容易出错，简单就代表好升级，简单就代表易于扩展等等。设计出好的协议的前提还是得对服务器开发有着深刻的理解和丰富的经验才行。&lt;/p&gt;

&lt;p&gt;这次也碰到了如何实现一个高效的UDP服务器的问题，我们都知道UDP没有连接机制，于是服务器只有一个socket fd用来收发UDP数据包，现在是多核时代，服务器都是2，30个cpu，用这么多的线程去操作一个共享的socket，也很难完全将服务器的资源全部利用起来。所以google为了解决他们的DNS服务器性能问题，就搞出了REUSEPORT这个内核patch，它已经出现在了3.9的内核中。当然reuseport对UDP/TCP都有效的，但个人觉得对于长连接的TCP服务器来说效果不见得明显，但是对于UDP和短连接为主的tcp来说性能应该是有质的提高才对，回头我将详细的写篇文章介绍reuseport的测试情况。&lt;/p&gt;

&lt;p&gt;再提一下Google的edns0-client-subnet协议，这货就是Google用来扩展DNS协议，觉得DNS协议不够用，不够用主要还是体现在CDN的DNS调度器这种场景下。client-subnet的目的就是让一个DNS查询请求携带上客户端IP，以供CDN做出精确调度使用。Google是一家喜欢制定标准的公司，它有魄力和野心去定义互联网。&lt;/p&gt;

&lt;p&gt;重写完DNS调度器的时候，发现自己在C程序设计上面，深受了Nginx的影响了。在没有接触Nginx之前，对模块化开发只能说是非常初级的地步，甚至是根本不懂什么是C程序的模块化开发。C语言是一门语法上太简单的语言，没有太多的工具给你使用，因此C语言最难的地方应该就是如何用有限的工具堆砌出一个优雅的程序。用C语言设计出高可扩展的程序不是一件易事，这次调度器的开发加深了自己对模块化设计的掌握。编程真的需要有一点追求，才能迫使自己做出更好的设计，本次开发中为了调整一个模块的代码风格，一直搞到凌晨3点才完事，可能你觉得我是在自找苦吃，其实我真的是在享受这个过程，哈哈。很多人都认为编程语言不重要，但编程语言真的决定了你的思考方式。我喜欢C和Go语言，你呢？&lt;/p&gt;

&lt;p&gt;最后，我想说一下代码风格，我坚定的认为一个项目、一份代码、一个程序应该采用统一的风格，不管是多少个人一起完成的，大家都应该严格的遵守。程序员真的是一个非常怪异的群体，每个人都有自己的风格或者说是性格，不喜欢受规矩束缚，更不喜欢被人指指点点，不管多么的不喜欢，我还是坚定的认为一起做一件事情遵循同一个规范是一件很重要的事情。目前正努力学习让自己可以随时融入到任何一种规范、做事风格之中。&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>reuseport</title>
   <link href="http://skoo87.github.io/system/2014/03/05/reuseport"/>
   <updated>2014-03-05T00:00:00+08:00</updated>
   <id>http://skoo87.github.io/system/2014/03/05/reuseport</id>
   <content type="html"></content>
 </entry>
 
 <entry>
   <title>goroutine与调度器</title>
   <link href="http://skoo87.github.io/go/2013/11/29/golang-schedule"/>
   <updated>2013-11-29T00:00:00+08:00</updated>
   <id>http://skoo87.github.io/go/2013/11/29/golang-schedule</id>
   <content type="html">&lt;p&gt;我们都知道Go语言是原生支持语言级并发的，这个并发的最小逻辑单元就是goroutine。goroutine就是Go语言提供的一种用户态线程，当然这种用户态线程是跑在内核级线程之上的。当我们创建了很多的goroutine，并且它们都是跑在同一个内核线程之上的时候，就需要一个调度器来维护这些goroutine，确保所有的goroutine都使用cpu，并且是尽可能公平的使用cpu资源。&lt;/p&gt;

&lt;p&gt;这个调度器的原理以及实现值得我们去深入研究一下。支撑整个调度器的主要有4个重要结构，分别是M、G、P、Sched，前三个定义在runtime.h中，Sched定义在proc.c中。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Sched结构就是调度器，它维护有存储M和G的队列以及调度器的一些状态信息等。&lt;/li&gt;

&lt;li&gt;M代表内核级线程，一个M就是一个线程，goroutine就是跑在M之上的；M是一个很大的结构，里面维护小对象内存cache（mcache）、当前执行的goroutine、随机数发生器等等非常多的信息。&lt;/li&gt;

&lt;li&gt;P全称是Processor，处理器，它的主要用途就是用来执行goroutine的，所以它也维护了一个goroutine队列，里面存储了所有需要它来执行的goroutine，这个P的角色可能有一点让人迷惑，一开始容易和M冲突，后面重点聊一下它们的关系。&lt;/li&gt;

&lt;li&gt;G就是goroutine实现的核心结构了，G维护了goroutine需要的栈、程序计数器以及它所在的M等信息。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;理解M、P、G三者的关系对理解整个调度器非常重要，我从网络上找了一个图来说明其三者关系：&lt;/p&gt;
&lt;div align=''&gt;
&lt;img height='100' src='/assets/images/golang-car.jpeg' width='100' /&gt;
&lt;/div&gt;
&lt;p&gt;地鼠(gopher)用小车运着一堆待加工的砖。M就可以看作图中的地鼠，P就是小车，G就是小车里装的砖。一图胜千言啊，弄清楚了它们三者的关系，下面我们就开始重点聊地鼠是如何在搬运砖块的。&lt;/p&gt;

&lt;h5 id='id15'&gt;启动过程&lt;/h5&gt;

&lt;p&gt;在关心绝大多数程序的内部原理的时候，我们都试图去弄明白其启动初始化过程，弄明白这个过程对后续的深入分析至关重要。在asm_amd64.s文件中的汇编代码_rt0_amd64就是整个启动过程，核心过程如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CALL	runtime·args(SB)
CALL	runtime·osinit(SB)
CALL	runtime·hashinit(SB)
CALL	runtime·schedinit(SB)

// create a new goroutine to start program
PUSHQ	$runtime·main·f(SB)		// entry
PUSHQ	$0			// arg size
CALL	runtime·newproc(SB)
POPQ	AX
POPQ	AX

// start this M
CALL	runtime·mstart(SB)&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动过程做了调度器初始化runtime·schedinit后，调用runtime·newproc创建出第一个goroutine，这个goroutine将执行的函数是runtime·main，这第一个goroutine也就是所谓的主goroutine。我们写的最简单的Go程序&amp;#8221;hello，world&amp;#8221;就是完全跑在这个goroutine里，当然任何一个Go程序的入口都是从这个goroutine开始的。最后调用的runtime·mstart就是真正的执行上一步创建的主goroutine。&lt;/p&gt;

&lt;p&gt;启动过程中的调度器初始化runtime·schedinit函数主要根据用户设置的GOMAXPROCS值来创建一批小车(P)，不管GOMAXPROCS设置为多大，最多也只能创建256个小车(P)。这些小车(p)初始创建好后都是闲置状态，也就是还没开始使用，所以它们都放置在调度器结构(Sched)的&lt;code&gt;pidle&lt;/code&gt;字段维护的链表中存储起来了，以备后续之需。&lt;/p&gt;

&lt;p&gt;查看runtime·main函数可以了解到主goroutine开始执行后，做的第一件事情是创建了一个新的内核线程(地鼠M)，不过这个线程是一个特殊线程，它在整个运行期专门负责做特定的事情——系统监控(sysmon)。接下来就是进入Go程序的main函数开始Go程序的执行。&lt;/p&gt;

&lt;p&gt;至此，Go程序就被启动起来开始运行了。一个真正干活的Go程序，一定创建有不少的goroutine，所以在Go程序开始运行后，就会向调度器添加goroutine，调度器就要负责维护好这些goroutine的正常执行。&lt;/p&gt;

&lt;h5 id='goroutineg'&gt;创建goroutine(G)&lt;/h5&gt;

&lt;p&gt;在Go程序中，时常会有类似代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go do_something()&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;go关键字就是用来创建一个goroutine的，后面的函数就是这个goroutine需要执行的代码逻辑。go关键字对应到调度器的接口就是&lt;code&gt;runtime·newproc&lt;/code&gt;。runtime·newproc干的事情很简单，就负责制造一块砖(G)，然后将这块砖(G)放入当前这个地鼠(M)的小车(P)中。&lt;/p&gt;

&lt;p&gt;每个新的goroutine都需要有一个自己的栈，G结构的&lt;code&gt;sched&lt;/code&gt;字段维护了栈地址以及程序计数器等信息，这是最基本的调度信息，也就是说这个goroutine放弃cpu的时候需要保存这些信息，待下次重新获得cpu的时候，需要将这些信息装载到对应的cpu寄存器中。&lt;/p&gt;

&lt;p&gt;假设这个时候已经创建了大量的goroutne，就轮到调度器去维护这些goroutine了。&lt;/p&gt;

&lt;h5 id='m'&gt;创建内核线程(M)&lt;/h5&gt;
&lt;div align=''&gt;
&lt;img height='100' src='/assets/images/golang-car.jpeg' width='100' /&gt;
&lt;/div&gt;
&lt;p&gt;Go程序中没有语言级的关键字让你去创建一个内核线程，你只能创建goroutine，内核线程只能由runtime根据实际情况去创建。runtime什么时候创建线程？以地鼠运砖图来讲，砖(G)太多了，地鼠(M)又太少了，实在忙不过来，刚好还有空闲的小车(P)没有使用，那就从别处再借些地鼠(M)过来直到把小车(p)用完为止。这里有一个地鼠(M)不够用，从别处借地鼠(M)的过程，这个过程就是创建一个内核线程(M)。创建M的接口函数是:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void newm(void (*fn)(void), P *p)&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;newm函数的核心行为就是调用clone系统调用创建一个内核线程，每个内核线程的开始执行位置都是runtime·mstart函数。参数p就是一辆空闲的小车(p)。&lt;/p&gt;

&lt;p&gt;每个创建好的内核线程都从runtime·mstart函数开始执行了，它们将用分配给自己小车去搬砖了。&lt;/p&gt;

&lt;h5 id='id16'&gt;调度核心&lt;/h5&gt;

&lt;p&gt;newm接口只是给新创建的M分配了一个空闲的P，也就是相当于告诉借来的地鼠(M)——“接下来的日子，你将使用1号小车搬砖，记住是1号小车；待会自己到停车场拿车。”，地鼠(M)去拿小车(P)这个过程就是&lt;code&gt;acquirep&lt;/code&gt;。runtime·mstart在进入&lt;code&gt;schedule&lt;/code&gt;之前会给当前M装配上P，runtime·mstart函数中的代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;} else if(m != &amp;amp;runtime·m0) {
	acquirep(m-&amp;gt;nextp);
	m-&amp;gt;nextp = nil;
}
schedule();&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;if分支的内容就是为当前M装配上P，&lt;code&gt;nextp&lt;/code&gt;就是newm分配的空闲小车(P)，只是到这个时候才真正拿到手罢了。没有P，M是无法执行goroutine的，就像地鼠没有小车无法运砖一样的道理。对应acquirep的动作是releasep，把M装配的P给载掉；活干完了，地鼠需要休息了，就把小车还到停车场，然后睡觉去。&lt;/p&gt;

&lt;p&gt;地鼠(M)拿到属于自己的小车(P)后，就进入工场开始干活了，也就是上面的&lt;code&gt;schedule&lt;/code&gt;调用。简化schedule的代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;static void
schedule(void)
{
	G *gp;

	gp = runqget(m-&amp;gt;p);
	if(gp == nil)
		gp = findrunnable();

	if (m-&amp;gt;p-&amp;gt;runqhead != m-&amp;gt;p-&amp;gt;runqtail &amp;amp;&amp;amp;
		runtime·atomicload(&amp;amp;runtime·sched.nmspinning) == 0 &amp;amp;&amp;amp;
		runtime·atomicload(&amp;amp;runtime·sched.npidle) &amp;gt; 0)  // TODO: fast atomic
		wakep();

	execute(gp);
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;schedule函数被我简化了太多，主要是我不喜欢贴大段大段的代码，因此只保留主干代码了。这里涉及到4大步逻辑：&lt;/p&gt;
&lt;div align=''&gt;
&lt;img height='200' src='/assets/images/gopher-bz.jpg' width='450' /&gt;
&lt;/div&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;runqget&lt;/code&gt;, 地鼠(M)试图从自己的小车(P)取出一块砖(G)，当然结果可能失败，也就是这个地鼠的小车已经空了，没有砖了。&lt;/li&gt;

&lt;li&gt;&lt;code&gt;findrunnable&lt;/code&gt;, 如果地鼠自己的小车中没有砖，那也不能闲着不干活是吧，所以地鼠就会试图跑去工场仓库取一块砖来处理；工场仓库也可能没砖啊，出现这种情况的时候，这个地鼠也没有偷懒停下干活，而是悄悄跑出去，随机盯上一个小伙伴(地鼠)，然后从它的车里试图偷一半砖到自己车里。如果多次尝试偷砖都失败了，那说明实在没有砖可搬了，这个时候地鼠就会把小车还回停车场，然后&lt;code&gt;睡觉&lt;/code&gt;休息了。如果地鼠睡觉了，下面的过程当然都停止了，地鼠睡觉也就是线程sleep了。&lt;/li&gt;

&lt;li&gt;&lt;code&gt;wakep&lt;/code&gt;, 到这个过程的时候，可怜的地鼠发现自己小车里有好多砖啊，自己根本处理不过来；再回头一看停车场居然有闲置的小车，立马跑到宿舍一看，你妹，居然还有小伙伴在睡觉，直接给屁股一脚，“你妹，居然还在睡觉，老子都快累死了，赶紧起来干活，分担点工作。”，小伙伴醒了，拿上自己的小车，乖乖干活去了。有时候，可怜的地鼠跑到宿舍却发现没有在睡觉的小伙伴，于是会很失望，最后只好向工场老板说——&amp;#8221;停车场还有闲置的车啊，我快干不动了，赶紧从别的工场借个地鼠来帮忙吧。&amp;#8221;，最后工场老板就搞来一个新的地鼠干活了。&lt;/li&gt;

&lt;li&gt;&lt;code&gt;execute&lt;/code&gt;，地鼠拿着砖放入火种欢快的烧练起来。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;注： “地鼠偷砖”叫&lt;a href='http://supertech.csail.mit.edu/papers/steal.pdf'&gt;work stealing&lt;/a&gt;，一种调度算法。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;到这里，貌似整个工场都正常的运转起来了，无懈可击的样子。不对，还有一个疑点没解决啊，假设地鼠的车里有很多砖，它把一块砖放入火炉中后，何时把它取出来，放入第二块砖呢？难道要一直把第一块砖烧练好，才取出来吗？那估计后面的砖真的是等得花儿都要谢了。这里就是要真正解决goroutine的调度，上下文切换问题。&lt;/p&gt;

&lt;h5 id='id17'&gt;调度点&lt;/h5&gt;

&lt;p&gt;当我们翻看channel的实现代码可以发现，对channel读写操作的时候会触发调用runtime·park函数。goroutine调用park后，这个goroutine就会被设置位waiting状态，放弃cpu。被park的goroutine处于waiting状态，并且这个goroutine不在小车(P)中，如果不对其调用runtime·ready，它是永远不会再被执行的。除了channel操作外，定时器中，网络poll等都有可能park goroutine。&lt;/p&gt;

&lt;p&gt;除了park可以放弃cpu外，调用runtime·gosched函数也可以让当前goroutine放弃cpu，但和park完全不同；gosched是将goroutine设置为runnable状态，然后放入到调度器全局等待队列（也就是上面提到的工场仓库，这下就明白为何工场仓库会有砖块(G)了吧）。&lt;/p&gt;

&lt;p&gt;除此之外，就轮到系统调用了，有些系统调用也会触发重新调度。Go语言完全是自己封装的系统调用，所以在封装系统调用的时候，可以做不少手脚，也就是进入系统调用的时候执行entersyscall，退出后又执行exitsyscall函数。 也只有封装了entersyscall的系统调用才有可能触发重新调度，它将改变小车(P)的状态为syscall。还记一开始提到的sysmon线程吗？这个系统监控线程会扫描所有的小车(P)，发现一个小车(P)处于了syscall的状态，就知道这个小车(P)遇到了goroutine在做系统调用，于是系统监控线程就会创建一个新的地鼠(M)去把这个处于syscall的小车给抢过来，开始干活，这样这个小车中的所有砖块(G)就可以绕过之前系统调用的等待了。被抢走小车的地鼠等系统调用返回后，发现自己的车没，不能继续干活了，于是只能把执行系统调用的goroutine放回到工场仓库，自己&lt;code&gt;睡觉&lt;/code&gt;去了。&lt;/p&gt;

&lt;p&gt;从goroutine的调度点可以看出，调度器还是挺粗暴的，调度粒度有点过大，公平性也没有想想的那么好。总之，这个调度器还是比较简单的。&lt;/p&gt;

&lt;h5 id='id18'&gt;现场处理&lt;/h5&gt;

&lt;p&gt;goroutine在cpu上换入换出，不断上下文切换的时候，必须要保证的事情就是&lt;code&gt;保存现场&lt;/code&gt;和&lt;code&gt;恢复现场&lt;/code&gt;，保存现场就是在goroutine放弃cpu的时候，将相关寄存器的值给保存到内存中；恢复现场就是在goroutine重新获得cpu的时候，需要从内存把之前的寄存器信息全部放回到相应寄存器中去。&lt;/p&gt;

&lt;p&gt;goroutine在主动放弃cpu的时候(park/gosched)，都会涉及到调用runtime·mcall函数，此函数也是汇编实现，主要将goroutine的栈地址和程序计数器保存到G结构的&lt;code&gt;sched&lt;/code&gt;字段中，mcall就完成了现场保存。恢复现场的函数是runtime·gogocall，这个函数主要在&lt;code&gt;execute&lt;/code&gt;中调用，就是在执行goroutine前，需要重新装载相应的寄存器。&lt;/p&gt;
&lt;br /&gt;&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;人也许总得有一点遗憾才完美&lt;/em&gt;&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>使用cpu的时钟周期作为随机数发生器的种子</title>
   <link href="http://skoo87.github.io/system/2013/11/14/rdtsc-instruction"/>
   <updated>2013-11-14T00:00:00+08:00</updated>
   <id>http://skoo87.github.io/system/2013/11/14/rdtsc-instruction</id>
   <content type="html">&lt;p&gt;对于一个伪随机数发生器来说，种子的设置是非常重要的；Go语言runtime中的每个线程也有自己的一个随机数发生器，当然也是伪的，这个伪随机数发生器的种子设置采用了另外一种方法——使用了cpu的时钟周期计数器。&lt;/p&gt;

&lt;p&gt;go语言的实现采用了一段汇编代码读取cpu的cycle信息，如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;TEXT runtime·cputicks(SB),7,$0
	RDTSC
	SHLQ	$32, DX
	ADDQ	DX, AX
	RET&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这段汇编代码的语法好像有点奇怪，和平时常见的AT&amp;amp;T语法有那么一点不同；确实，Go语言有一套自己的编译器，汇编器当然也是自己的了。这套编译器其实是Plan9平台上的玩意，个人觉得这汇编语法更顺眼一点，相比AT&amp;amp;T来说。&lt;/p&gt;

&lt;p&gt;这里采用汇编实现的函数runtime·cputicks，第一条就是rdtsc指令。rdtsc指令就是核心，它是用来获取cpu自从加电以来执行的周期数，读取到的64位整数的低32位保存在AX寄存器，高32位保存到DX寄存器。所以，接下来的SHLQ指令就是将DX中的高32位值左移32位，然后通过ADD指令和低32位值加起来就得到了cpu的cycle值了。&lt;/p&gt;

&lt;p&gt;可以看出使用cpu的时钟周期来作为随机发生器的种子是一种高精度的方法，比采用当前时间戳的精度要高很多。也有不少人采用rdtsc指令来做代码的性能测量，也就是在执行之前读取一下cpu cycle，完成后再读取一次，最后求一下差值。使用rdtsc做性能profiling在现在这些高端cpu上可能不那么靠谱了，主要是因为cpu支持指令乱序执行的原因，当然也还有一些其他的原因，可以看这篇文章：&lt;a href='http://www.ccsl.carleton.ca/~jamuir/rdtscpm1.pdf'&gt;http://www.ccsl.carleton.ca/~jamuir/rdtscpm1.pdf&lt;/a&gt; 。&lt;/p&gt;

&lt;p&gt;对了，rdtsc是read time stamp counter的缩写。&lt;/p&gt;
&lt;br /&gt;&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;孤陋寡闻，第一次见这指令的使用，好久不写博客了，随手记录一下。多关注，留意细节可以学到蛮多意想不到的东西。&lt;/em&gt;&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>Go runtime中的那些东东</title>
   <link href="http://skoo87.github.io/go/2013/11/11/go-runtime-mind"/>
   <updated>2013-11-11T00:00:00+08:00</updated>
   <id>http://skoo87.github.io/go/2013/11/11/go-runtime-mind</id>
   <content type="html">&lt;p&gt;好久没更新了，贴一个最近随手画的runtime思维导图，顺便祝贺一下我厂双11成交又创新高。&lt;/p&gt;

&lt;p&gt;(我会逐渐细化这张思维导图)&lt;/p&gt;
&lt;img height='600' src='/assets/images/Go-runtime.png' width='800' /&gt;</content>
 </entry>
 
 <entry>
   <title>Go语言内存分配器的实现</title>
   <link href="http://skoo87.github.io/go/2013/10/13/go-memory-manage-system-alloc"/>
   <updated>2013-10-13T00:00:00+08:00</updated>
   <id>http://skoo87.github.io/go/2013/10/13/go-memory-manage-system-alloc</id>
   <content type="html">&lt;p&gt;前几天断断续续的写了3篇关于Go语言内存分配器的文章，分别是&lt;a href='http://www.bigendian123.com/go/2013/10/08/go-memory-manage-system-design/'&gt;Go语言内存分配器设计&lt;/a&gt;、&lt;a href='http://www.bigendian123.com/go/2013/10/09/go-memory-manage-system-fixalloc/'&gt;Go语言内存分配器-FixAlloc&lt;/a&gt;、&lt;a href='http://www.bigendian123.com/go/2013/10/11/go-memory-manage-system-span/'&gt;Go语言内存分配器-MSpan&lt;/a&gt;，这3篇主要是本文的前戏，其实所有的内容本可以在一篇里写完的，但内容实在太多了，没精力一口气搞定。本文将把整个内存分配器的架构以及核心组件给详细的介绍一下，当然亲自对照着翻看一下代码才是王道。&lt;/p&gt;

&lt;p&gt;内存布局结构图&lt;/p&gt;
&lt;div align='center'&gt;
&lt;img height='350' src='/assets/images/goalloc.png' width='500' /&gt;
&lt;/div&gt;
&lt;p&gt;我把整个核心代码的逻辑给抽象绘制出了这个内存布局图，它基本展示了Go语言内存分配器的整体结构以及部分细节（这结构图应该同样适用于tcmalloc）。从此结构图来看，内存分配器还是有一点小复杂的，但根据具体的逻辑层次可以拆成三个大模块——cache，central，heap，然后一个一个的模块分析下去，逻辑就显得特别清晰明了了。位于结构图最下边的&lt;code&gt;Cache&lt;/code&gt;就是cache模块部分；central模块对应深蓝色部分的&lt;code&gt;MCentral&lt;/code&gt;，central模块的逻辑结构很简单，所以结构图就没有详细的绘制了；&lt;code&gt;Heap&lt;/code&gt;是结构图中的核心结构，对应heap模块，也可以看出来central是直接被Heap管理起来的，属于Heap的子模块。&lt;/p&gt;

&lt;p&gt;在分析内存分配器这部分源码的时候，首先需要明确的是所有内存分配的入口，有了入口就可以从这里作为起点一条线的看下去，不会有太大的障碍。这个入口就是malloc.goc源文件中的&lt;code&gt;runtime·mallocgc&lt;/code&gt;函数，这个入口函数的主要工作就是分配内存以及触发gc(本文将只介绍内存分配)，在进入真正的分配内存之前，此入口函数还会判断请求的是小内存分配还是大内存分配(32k作为分界线)；小内存分配将调用&lt;code&gt;runtime·MCache_Alloc&lt;/code&gt;函数从Cache获取，而大内存分配调用&lt;code&gt;runtime·MHeap_Alloc&lt;/code&gt;直接从Heap获取。入口函数过后，就会真正的进入到具体的内存分配过程中去了。&lt;/p&gt;

&lt;p&gt;在真正进入内存分配过程之前，还需要了解一下整个内存分配器是如何创建的以及初始化成什么样子。完成内存分配器创建初始化的函数是&lt;code&gt;runtime·mallocinit&lt;/code&gt;，看一下简化的源码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void
runtime·mallocinit(void)
{
	// 创建mheap对象，这个是直接从操作系统分配内存。heap是全局的，所有线程共享，一个Go进程里只有一个heap。
	if((runtime·mheap = runtime·SysAlloc(sizeof(*runtime·mheap))) == nil)
		runtime·throw(&amp;quot;runtime: cannot allocate heap metadata&amp;quot;);

	// 64位平台，申请一大块内存地址保留区，后续所有page的申请都会从这个地址区里分配。这个区就是结构图中的arena。 
	if(sizeof(void*) == 8 &amp;amp;&amp;amp; (limit == 0 || limit &amp;gt; (1&amp;lt;&amp;lt;30))) {
		arena_size = MaxMem;
		bitmap_size = arena_size / (sizeof(void*)*8/4);
		p = runtime·SysReserve((void*)(0x00c0ULL&amp;lt;&amp;lt;32), bitmap_size + arena_size);
	}

	// 初始化好heap的arena以及bitmap。
	runtime·mheap-&amp;gt;bitmap = p;
	runtime·mheap-&amp;gt;arena_start = p + bitmap_size;
	runtime·mheap-&amp;gt;arena_used = runtime·mheap-&amp;gt;arena_start;
	runtime·mheap-&amp;gt;arena_end = runtime·mheap-&amp;gt;arena_start + arena_size;

	// 初始化heap的其他内部结构，如：spanalloc、cacachealloc都FixAlloc的初始化，free、large字段都是挂载维护span的双向循环链表。
	runtime·MHeap_Init(runtime·mheap, runtime·SysAlloc);
	
	// 从heap的cachealloc从分配MCache，挂在一个线程上。
	m-&amp;gt;mcache = runtime·allocmcache();
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;初始化过程主要是在折腾mcache和mheap两个部分，而mcentral在实际逻辑中是属于mheap的子模块，所以初始化过程就没明确的体现出来，这和我绘制的结构图由两大块构造相对应。&lt;strong&gt;heap是所有底层线程共享的；而cache是每个线程都分别拥有一个，是独享的。在64位平台，heap从操作系统申请的内存地址保留区只有136G，其中bitmap需要8G空间，因此真正可申请的内存就是128G。当然128G在绝大多数情况都是够用的，但我所知道的还是有个别特殊应用的单机内存是超过128G的。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;下面按小内存分配的处理路径，从cache到central到heap的过程详细介绍一下。&lt;/p&gt;

&lt;h4 id='cache'&gt;Cache&lt;/h4&gt;

&lt;p&gt;cache的实现主要在mcache.c源文件中，结构MCache定义在malloc.h中，从cache中申请内存的函数原型:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void *runtime·MCache_Alloc(MCache *c, int32 sizeclass, uintptr size, int32 zeroed)&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数&lt;code&gt;size&lt;/code&gt;是需要申请的内存大小，需要知道的是这个size并不一定是我们申请内存的时候指定的大小，一般来说它会稍大于指定的大小。从结构图上可以清晰看到cache有一个0到n的list数组，list数组的每个单元挂载的是一个链表，链表的每个节点就是一块可用的内存，同一链表中的所有节点内存块都是大小相等的；但是不同链表的内存大小是不等的，也就是说list数组的一个单元存储的是一类固定大小的内存块，不同单元里存储的内存块大小是不等的。这就说明cache缓存的是不同类大小的内存对象，当然想申请的内存大小最接近于哪类缓存内存块时，就分配哪类内存块。list数组的0到n下标就是不同的sizeclass，n是一个固定值等于60，所以cache能够提供60类(0&amp;#60;sizeclass&amp;#60;61)不同大小的内存块。这60类大小是如何划分的，可以查看msize.c源文件。&lt;/p&gt;

&lt;p&gt;runtime·MCache_Alloc分配内存的过程是，根据参数sizeclass从list数组中取出一个内存块链表，如果这个链表不为空，就直接把第一个节点返回即可；如果链表是空，说明cache中没有满足此类大小的缓存内存，这个时候就调用&lt;code&gt;runtime·MCentral_AllocList&lt;/code&gt;从central中获取 &lt;font color='red'&gt;**一批**&lt;/font&gt; 此类大小的内存块，再把第一个节点返回使用，其他剩余的内存块挂到这个链表上，为下一次分配做好缓存。&lt;/p&gt;

&lt;p&gt;cache上的内存分配逻辑很简单，就是cache取不到就到central中去取。除了内存的分配外，cache上还存在很多的状态计数器，主要是用来统计内存的分配情况，比如：分配了多少内存，缓存了多少内存等等。这些状态计数器非常重要，可以用来监控我们程序的内存管理情况以及profile等，runtime包里的&lt;code&gt;MemStats&lt;/code&gt;类的数据就是来自这些底层的计数器。&lt;/p&gt;

&lt;p&gt;cache的释放条件主要有两个，一是当某个内存块链表过长（&amp;gt;=256）时，就会截取此链表的一部分节点，返还给central；二是整个cache缓存的内存过大(&amp;gt;=1M)，同样将每个链表返还一部分节点给central。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;cache层在进行内存分配和释放操作的时候，是没有进行加锁的，也不需要加锁，因为cache是每个线程独享的。所以cache层的主要目的就是提高小内存的频繁分配释放速度。&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id='central'&gt;Central&lt;/h4&gt;

&lt;p&gt;malloc.h源文件里MHeap结构关于central的定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;struct MHeap
{
	。。。
	
	struct {
		MCentral;
		byte pad[CacheLineSize];
	} central[NumSizeClasses];
	
	。。。
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;结合结构图可以看出，在heap结构里，使用了一个0到n的数组来存储了一批central，并不是只有一个central对象。从上面结构定义可以知道这个数组长度位61个元素，也就是说heap里其实是维护了61个central，这61个central对应了cache中的list数组，也就是每一个sizeclass就有一个central。所以，在cache中申请内存时，如果在某个sizeclass的内存链表上找不到空闲内存，那么cache就会向对应的sizeclass的central获取一批内存块。注意，这里central数组的定义里面使用填充字节，这是因为多线程会并发访问不同central避免false sharing。&lt;/p&gt;

&lt;p&gt;central的实现主要在mcentral.c源文件，核心结构MCentral定义在malloc.h中，结构如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;struct MCentral
{
	Lock;
	int32 sizeclass;
	MSpan nonempty;
	MSpan empty;
	int32 nfree;
};&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;MCentral结构中的nonempty和empty字段是比较重要的，重点解释一下这两个字段。这两字段都是MSpan类型，大胆的猜测一下这两个字段将分别挂一个span节点构造的双向链表（MSpan在上一篇文章详细介绍了），只是这个双向链表的头节点不作使用罢了。&lt;code&gt;nonempty&lt;/code&gt;字面意思是非空，表示这个链表上存储的span节点都是非空状态，也就是说这些span节点都有空闲的内存；&lt;code&gt;empty&lt;/code&gt;表示此链表存储的span都是空的，它们都没有空闲可用的内存了。其实一开始empty链表就是空的，是当nonempty上的的一个span节点被用完后，就会将span移到empty链表中。&lt;/p&gt;

&lt;p&gt;我们知道cache在内存不足的时候，会通过&lt;code&gt;runtime·MCentral_AllocList&lt;/code&gt;从central获取一批内存块，central其实也只有cache一个上游用户，看一下此函数的简化逻辑：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int32
runtime·MCentral_AllocList(MCentral *c, int32 n, MLink **pfirst)
{
	runtime·lock(c);

	// 第一部分：判断nonempty是否为空，如果是空的话，就需要从heap中获取span来填充nonempty链表了。		
	// Replenish central list if empty.
	if(runtime·MSpanList_IsEmpty(&amp;amp;c-&amp;gt;nonempty)) {
		if(!MCentral_Grow(c)) {
			。。。。。。。
		}
	}

	// 第二部分：从nonempty链表上取一个span节点，然后从span的freelist里获取足够的内存块。这个span内存不足时，就有多少拿多少了。
	s = c-&amp;gt;nonempty.next;
	cap = (s-&amp;gt;npages &amp;lt;&amp;lt; PageShift) / s-&amp;gt;elemsize;
	avail = cap - s-&amp;gt;ref;
	if(avail &amp;lt; n)
		n = avail;

	// First one is guaranteed to work, because we just grew the list.
	first = s-&amp;gt;freelist;
	last = first;
	for(i=1; i&amp;lt;n; i++) {
		last = last-&amp;gt;next;
	}
	s-&amp;gt;freelist = last-&amp;gt;next;
	last-&amp;gt;next = nil;
	s-&amp;gt;ref += n;
	c-&amp;gt;nfree -= n;

	// 第三部分：如果上面的span内存被取完了，就将它移到empty链表中去。
	if(n == avail) {
		runtime·MSpanList_Remove(s);
		runtime·MSpanList_Insert(&amp;amp;c-&amp;gt;empty, s);
	}

	runtime·unlock(c);
	
	// 第四部分：最后将取得的内存块链通过参数pfirst返回。
	*pfirst = first;
	return n;
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从central中获取一批内存块交给cache的过程，看上去也不怎么复杂，只是这个过程是需要加锁的。这里重点要关注第一部分填充nonempty的情况，也就是central没有空闲内存，需要向heap申请。这里调用的是&lt;code&gt;MCentral_Grow&lt;/code&gt;函数。MCentral_Grow函数的主要工作是，首先调用runtime·MHeap_Alloc函数向heap申请一个span；然后将span里的连续page给切分成central对应的sizeclass的小内存块，并将这些小内存串成链表挂在span的freelist上；最后将span放入到nonempty链表中。&lt;strong&gt;central在无空闲内存的时候，向heap只要了一个span，不是多个；申请的span含多少page是根据central对应的sizeclass来确定。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;central中的内存分配过程搞定了，看一下大概的释放过程。cache层在释放内存的时候，是将一批小内存块返还给central，central在收到这些归还的内存块的时候，会把每个内存块分别还给对应的span。在把内存块还给span后，如果span先前是被用完了内存，待在empty链表中，那么此刻就需要将它移动到nonempty中，表示又有可用内存了。在归还小内存块给span后，如果span中的所有page内存都收回来了，也就是没有任何内存被使用了，此刻就会将这个span整体归还给heap了。&lt;/p&gt;

&lt;p&gt;在central这一层，内存的管理粒度基本就是span了，所以span是非常重要的一个工具组件。&lt;/p&gt;

&lt;h5 id='heap'&gt;Heap&lt;/h5&gt;

&lt;p&gt;总算来到了最大的一层heap，这是离Go程序最远的一层，离操作系统最近的一层，这一层主要就是从操作系统申请内存交给central等。heap的核心结构MHeap定义在malloc.h中，一定要细看。不管是通过central从heap获取内存，还是大内存情况跳过了cache和central直接向heap要内存，都是调用如下函数来请求内存的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MSpan* runtime·MHeap_Alloc(MHeap *h, uintptr npage, int32 sizeclass, int32 acct, int32 zeroed)&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;函数原型可以看出，向heap要内存，不是以字节为单位，而是要多少个page。参数npage就是需要的page数量，sizeclass等于0，就是绕过cache和central的直接找heap的大内存分配；central调用此函数时，sizecalss一定是1到60的一个值。从heap中申请到的所有page肯定都是连续的，并且是通过span来管理的，所以返回值是一个span，不是page数组等。&lt;/p&gt;

&lt;p&gt;真正在heap中执行内存分配逻辑的是位于mheap.c中的&lt;code&gt;MHeap_AllocLocked&lt;/code&gt;函数。分析此函数的逻辑前，先看一下结构图中heap的free和large两个域。&lt;code&gt;free&lt;/code&gt;是一个256个单元的数组，每个单元上存储的都是一个span链表；但是不同的单元span含page的个数也是不同的，span含page的个数等于此span所在单元的下标，比如：free[5]中的span都含有5个page。如果一个span的page数超过了255，那这个span就会被放到&lt;code&gt;large&lt;/code&gt;链表中。&lt;/p&gt;

&lt;p&gt;从heap中要内存，首先是根据请求的page数量到free或者large中获取一个最合适的span。当然，如果在large链表中都找不到合适的span，就只能调用MHeap_Grow函数从操作系统申请内存，然后填充Heap，再试图分配。拿到的span所含page数大于了请求的page数的的时候，并不会将整个span返回使用，而是对这个span进行拆分，拆为两个span，将剩余span重新放回到free或者large链表中去。因为heap面对的始终是page，如果全部返回使用，可能就会太浪费内存了，所以这里只返回请求的page数是有必要的。&lt;/p&gt;

&lt;p&gt;从heap中请求出去的span，在遇到内存释放退还给heap的时候，主要也是将span放入到free或者large链表中。&lt;/p&gt;

&lt;p&gt;heap比骄复杂的不是分配释放内存，而是需要维护很多的元数据，比如结构图还没有介绍的map域，这个map维护的就是page到span的映射，也就是任何一块内存在算出page后，就可以知道这块内存属于哪个span了，这样才能做到正确的内存回收。除了map以外，还有bitmap等结构，用来标记内存，为gc服务。&lt;/p&gt;

&lt;p&gt;后面对垃圾收集器(gc)的分析时，还会回头来看heap。本文内容已经够多了，但确实还有很多的细节没有介绍到，比如：heap是如何从操作系统拿内存、heap中存在一个2分钟定时强制gc的goroutine等等。&lt;/p&gt;

&lt;p&gt;强烈建议熟悉C语言的，亲自看看源码，里面有太多有趣的细节了。&lt;/p&gt;

&lt;p&gt;注意：本文基于Go1.1.2版本代码。&lt;/p&gt;
&lt;br /&gt;&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;在C语言的世界里，内存管理是最头痛的事情，同时也是最酷的事情。&lt;/em&gt;&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>Go语言内存分配器-MSpan</title>
   <link href="http://skoo87.github.io/go/2013/10/11/go-memory-manage-system-span"/>
   <updated>2013-10-11T00:00:00+08:00</updated>
   <id>http://skoo87.github.io/go/2013/10/11/go-memory-manage-system-span</id>
   <content type="html">&lt;p&gt;MSpan和FixAlloc一样，都是内存分配器的基础工具组件，但和FixAlloc没太大的交集，各自发挥功效而已。span(MSpan简称span)是用来管理一组组page对象，先解释一下page，page就是一个4k大小的内存块而已。span就是将这一个个连续的page给管理起来，注意是连续的page，不是东一个西一个的乱摆设的page。为了直观形象的感受一下span，还是得画个图吧，图形是最好的交流语言。&lt;/p&gt;
&lt;div align='center'&gt;
&lt;img height='350' src='/assets/images/span.png' width='500' /&gt;
&lt;/div&gt;
&lt;p&gt;MSpan结构定义在malloc.h头文件中，代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;struct MSpan
{
	MSpan	*next;		// in a span linked list
	MSpan	*prev;		// in a span linked list
	PageID	start;		// starting page number
	uintptr	npages;		// number of pages in span
	MLink	*freelist;	// list of free objects
	uint32	ref;		// number of allocated objects in this span
	int32	sizeclass;	// size class
	uintptr	elemsize;	// computed from sizeclass or from npages
	uint32	state;		// MSpanInUse etc
	int64   unusedsince;	// First time spotted by GC in MSpanFree state
	uintptr npreleased;	// number of pages released to the OS
	byte	*limit;		// end of data in span
	MTypes	types;		// types of allocated objects in this span
};&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;span结构比较重要的字段，都出现在上面的结构图中了，当然并不是说其他的字段就不重要了。span的结构中有pre/next两个指针，一看就能猜到是用来构造双向链表的。没错，在实际的使用中，span确实是出现在双向循环链表中。span可能会用在分配小对象（小于等于32k）的过程中，也可能会用于分配大对象(大于32k)，在分配不同类型对象的时候，span管理的元数据也大不相同。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;npages&lt;/code&gt;表示是此span存储的page的个数（比如：上图中就画了3个page），&lt;code&gt;start&lt;/code&gt;可以看作一个page指针，指向第一个page，有了第一个page当然就可以算出后面的任何一个page的起始地址了，因为span管理的始终是连续的一组page。这里需要注意start的类型是PageID，由此可以看出这个start保存的并不是第一个page的起始地址，而是第一个page的id值。这个id值是如何算出来的呢？其实给每个page算一个id，是非常简单的事情，只要将这个page的的地址除以4096取整(伪代码：&lt;code&gt;page_addr&amp;gt;&amp;gt;20&lt;/code&gt;)即可，当然前提是已经保证好了每个page按4k对齐。是不是觉得很精妙，这样一来每个page都有一个整数id了，并且任何一个内存地址都可以通过移位算出这个地址属于哪个page，这个很重要。&lt;/p&gt;

&lt;p&gt;start是span最重要的一个字段，它维护好了所有的page。&lt;code&gt;sizeclass&lt;/code&gt;如果是0的话，就代表这个span是用来分配大对象的，其他值当然都是分配小对象了。在分配小对象的时候，start字段维护的所有page，最后将会被切分成一个一个的连续内存块，内存块大小当然就是小对象的大小，这些切分出来的内存块将被链接成为一个链表挂在freelist字段上。分配大对象的时候，freelist就没什么用了。&lt;/p&gt;

&lt;p&gt;span干的活，也就这么一点，反正就是管理一组连续的page。内存分配器中的每个page都会属于一个span，page永远不会独立存在。span相关的API有：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 初始化一个span结构，将分配的page放入到这个span中。
void	runtime·MSpan_Init(MSpan *span, PageID start, uintptr npages);

// 下面这些都是操作span构成的双向链表了。
void	runtime·MSpanList_Init(MSpan *list);
bool	runtime·MSpanList_IsEmpty(MSpan *list);
void	runtime·MSpanList_Insert(MSpan *list, MSpan *span);
void	runtime·MSpanList_Remove(MSpan *span);&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;span就先写到这里，接下来写mcache, mcentral, mheap等核心组件的时候还会涉及到如何使用span的。&lt;/p&gt;

&lt;p&gt;注：本文基于Go1.1.2版本代码&lt;/p&gt;
&lt;br /&gt;&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;写到这里，我感觉自己并没有将span写清楚了，写源码分析类技术文章真心比读源码有挑战，我试图在用最小的代码展示来将原理述说清楚。发现自己的功底还是不够啊。&lt;/em&gt;&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>Go语言内存分配器-FixAlloc</title>
   <link href="http://skoo87.github.io/go/2013/10/09/go-memory-manage-system-fixalloc"/>
   <updated>2013-10-09T00:00:00+08:00</updated>
   <id>http://skoo87.github.io/go/2013/10/09/go-memory-manage-system-fixalloc</id>
   <content type="html">&lt;p&gt;昨天写了一篇&lt;a href='http://www.bigendian123.com/go/2013/10/08/go-memory-manage-system-design/'&gt;Go语言内存分配器设计&lt;/a&gt;，记录了一下内存分配器的大体结构。在介绍内存分配器的核心实现前，本文先介绍一下内存分配器中一个工具组件——FixAlloc。FixAlloc称不上是核心组件，只是辅助实现整个内存分配器核心的一个基础工具罢了，由此可以看出FixAlloc还是一个比较重要的组件。引入FixAlloc的目的只是用来分配&lt;code&gt;MCache&lt;/code&gt;和&lt;code&gt;MSpan&lt;/code&gt;两个特定的对象，所以内存分配器中有&lt;code&gt;spanalloc&lt;/code&gt;和&lt;code&gt;cachealloc&lt;/code&gt;两个组件（见《Go语言内存分配器设计》的图）。MCache和MSpan两个结构在malloc.h中有定义。&lt;/p&gt;

&lt;p&gt;定义在malloc.h文件中的FixAlloc结构如下，比较关键的三个字段是alloc、list和chunk，其他的字段主要都是用来统计一些状态数据的，比如分配了多少内存之类。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;struct FixAlloc
{
	uintptr size;
	void *(*alloc)(uintptr);
	void (*first)(void *arg, byte *p);	// called first time p is returned
	void *arg;
	MLink *list;
	byte *chunk;
	uint32 nchunk;
	uintptr inuse;	// in-use bytes now
	uintptr sys;	// bytes obtained from system
};&lt;/code&gt;&lt;/pre&gt;
&lt;div align='center'&gt;
&lt;img height='350' src='/assets/images/fixalloc.jpg' width='500' /&gt;
&lt;/div&gt;
&lt;p&gt;FixAlloc的内存结构图，一看就很简单，简单到没有出现本文的必要了。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;list&lt;/code&gt;指针上挂的一个链表，这个链表的每个节点是一个固定大小的内存块，cachealloc中的list存储的内存块大小为&lt;code&gt;sizeof(MCache)&lt;/code&gt;，而spanalloc中的list存储的内存块大小为&lt;code&gt;sizeof(MSpan)&lt;/code&gt;。&lt;code&gt;chunk&lt;/code&gt;指针始终挂载的是一个128k大的内存块。&lt;/p&gt;

&lt;p&gt;FixAlloc提供了三个API，分别是runtime·FixAlloc_Init、runtime·FixAlloc_Alloc和runtime·FixAlloc_Free。&lt;/p&gt;

&lt;p&gt;分配一个mcache和mspan的伪代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MCache *mcache;
mcache = (MCache *) runtime·FixAlloc_Alloc(cachealloc);

MSpan *mspan;
mspan = (MSpan *) runtime·FixAlloc_Alloc(spanalloc);&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这段伪代码展示的是分配一个MCache和MSpan对象，内存分配器并不是直接使用malloc类函数向系统申请，而是走了FixAlloc。使用FixAlloc分配MCache和MSpan对象的时候，首先是查找FixAlloc的list链表，如果list不为空，就直接拿一个内存块返回使用; 如果list为空，就把焦点转移到chunk上去，如果128k的chunk内存中有足够的空间，就切割一块内存出来返回使用，如果chunk内存没有剩余内存的话，就从操作系统再申请128k内存替代老的chunk。FixAlloc的固定对象分配逻辑就这么简单，相反释放逻辑更简单了，释放的对象就是直接放到list中，并不会返回给操作系统。当然mcache的个数基本是稳定的，也就是底层线程个数，但span对象就不一定那么稳定了，所以FixAlloc的内存可能增长的因素就是span的对象太多。&lt;/p&gt;

&lt;p&gt;FixAlloc的实现位于mfixalloc.c文件中，代码目前还不到100行，实在是太简单了。本来是计划本文一起介绍完FixAlloc和MSpan两个基础组件，今天身体不舒服，小感冒了，没精力再写MSpan了。&lt;/p&gt;

&lt;p&gt;注：本文基于Go1.1.2版本。&lt;/p&gt;
&lt;br /&gt;&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;下班躺在床上，用小米盒子一边看着无聊的电视剧，一边抱着电脑写技术文章，貌似也可以让自己处于一种非常轻松的状态，前提可能是自己对需要写的东西了如指掌，不再需要去翻代码吧。&lt;/em&gt;&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>Go语言内存分配器设计</title>
   <link href="http://skoo87.github.io/go/2013/10/08/go-memory-manage-system-design"/>
   <updated>2013-10-08T00:00:00+08:00</updated>
   <id>http://skoo87.github.io/go/2013/10/08/go-memory-manage-system-design</id>
   <content type="html">&lt;p&gt;Go语言的整个内存管理子系统主要由两部分组成——内存分配器和垃圾收集器(gc)。十一小长假期为了避开我泱泱大国的人流高峰，于是在家宅了3天把Go语言的内存分配器部分的代码给研究了一番，总的来说还是非常酷的，自己也学到了不少的东西，就此记录分享一下。整个内存分配器完全是基于Google自家的tcmalloc的设计重新实现了一遍，因此，想看看Go语言的内存分配器实现的话，强烈建议先读一读&lt;a href='http://google-perftools.googlecode.com/svn/trunk/doc/tcmalloc.html'&gt;tcmalloc&lt;/a&gt;的介绍文档，然后看看Go runtime的malloc.h源码文件的注释介绍，这样基本就大概了解Go语言内存分配器的设计了。&lt;/p&gt;
&lt;div align='center'&gt;
&lt;img height='350' src='/assets/images/go-mem-system-design.png' width='500' /&gt;
&lt;/div&gt;
&lt;p&gt;Go的内存分配器主要也是解决小对象的分配管理和多线程的内存分配问题。（后面提到的内存分配器都是指代的Go语言实现的内存分配器）。内存分配器以32k作为对象大小的定夺标准，小于等于32k的内存对象一律视为小对象，而大于32k的对象就是大对象了。为何是32k作为分界线呢？这个我也不知道，我觉得这是一个经验值吧，如果你知道有其他更加科学的理由，麻烦告知我一下。&lt;/p&gt;

&lt;p&gt;内存分配器会将分配的小对象使用一个cache组件给缓存起来，只要是分配小对象就先到cache中查询一下，有空闲的内存就直接返回使用，不用向操作系统申请内存。内存分配器的这个cache组件可能同时存在多个，也就是每个实际线程都会有一个cache组件，这样一来，从cache里查询、获取空闲内存的时候就不需要加锁了，每次小对象的申请直接访问本线程对应的cache即可。我们再写程序的时候，其实绝大多数的内存申请都是小于32k的，属于小对象，因此这样的内存分配全部走本地cache，不用向操作系统申请显然是非常高效的。&lt;/p&gt;

&lt;p&gt;有cache，必然就有cache不命中的情况，内存分配器在面对&lt;code&gt;Cache&lt;/code&gt;查找不到空闲内存的时候，就会试图从&lt;code&gt;Central&lt;/code&gt;中申请一批小对象内存到本地缓存住，这里的Central是所有线程共享的一个组件，不是独占的，因此需要加锁操作。我们需要知道Central组件其实也是一个缓存，但它缓存的不是小对象内存块，而是一组一组的内存page(一个page占4k大小)。如果Central中没有缓存的空闲内存page的话，就从&lt;code&gt;Heap&lt;/code&gt;中申请内存来填充Central。当然对Heap的操作也是需要加锁，所有线程共享一个Heap。Heap中没有缓存的内存，当然就直接从操作系统拿取内存了。&lt;/p&gt;

&lt;p&gt;小对象的内存分配是通过一级一级的缓存来实现的，目的就是为了提升内存分配释放的速度以及避免内存碎片等问题。大于32k的大对象内存分配就没这么麻烦了，不用一层一层的查询各个缓存组件，而是直接向&lt;code&gt;Heap&lt;/code&gt;申请。上图是大概描述了一下整个内存分配器的组件结构，Cache、Central、Heap是三个核心组件，也是后面将重点分析的对象。&lt;/p&gt;

&lt;p&gt;内存分配器的实现我需要拆成多篇文章来写，没精力一口气写完，其实按组件拆开写也方便阅读嘛。后面的文章将陆续写完各个核心组件。&lt;/p&gt;
&lt;br /&gt;&lt;hr /&gt;
&lt;p&gt;题外话，在基础系统软件的世界里，内存管理是一个永恒的话题，所以存在tcmalloc和jemalloc这类非常优秀的内存分配器实现。据说，jemalloc在cpu核数较多的情况下，性能还要优于tcmalloc，但估计它们之间是不相伯仲的，主体设计都差不多。jemalloc也是纯C代码，应该是非常值得一看的。不知道为何，现在对C++项目，总有研究拖延症，没有强烈的动力去第一时间看源码。&lt;/p&gt;
&lt;br /&gt;
&lt;p&gt;&lt;em&gt;这两天，拜台风所赐，杭州又变大海了，家门口水深到我膝盖，不得不穿拖鞋短裤出门，真冷啊。附一个我今天在上班公交车上拍的照片吧。&lt;/em&gt;&lt;/p&gt;
&lt;div align='center'&gt;
&lt;img height='150' src='/assets/images/hz-boat.jpg' width='400' /&gt;
&lt;/div&gt;</content>
 </entry>
 
 <entry>
   <title>Base 128 Varint, 一种处理整数的变长二进制编码算法</title>
   <link href="http://skoo87.github.io/algorithm/2013/09/30/base-128-varint"/>
   <updated>2013-09-30T00:00:00+08:00</updated>
   <id>http://skoo87.github.io/algorithm/2013/09/30/base-128-varint</id>
   <content type="html">&lt;p&gt;我首次见到Base 128 Varint这个编码算法是在Google protobuf中，也不知道在它之前是否就已经出现了。最近，又碰到有人问如何对一个整数进行变长编码？也就是说给定的一个整数，能用1个字节存储就用一个字节的内存，需要两个字节存储就用两个字节的内存，而不是统一固定用4个字节或者8个字节。好处显然是非常明显的，特别是在二进制网络协议设计的时候，对整数进行变长编码很有必要。&lt;/p&gt;

&lt;p&gt;protobuf关于Base 128 Varint的介绍：&lt;a href='https://developers.google.com/protocol-buffers/docs/encoding'&gt;https://developers.google.com/protocol-buffers/docs/encoding&lt;/a&gt;，这个文档展示的例子是如何根据一个Base 128 Varint编码的二进制序列解码出整数。这里完整的介绍一下，如何用Base 128 Varint算法去编码一个整数，以及解码一个二进制序列。&lt;/p&gt;

&lt;p&gt;Base 128 Varint，为什么叫128？其实，就是因为只采用7bit的空间来存储有效数据，7bit当然最大只能存储128了。常规的一个byte是8个bit位，但在Base 128 Varint编码中，将最高的第8位用来作为一个标志位，如果这一位是1，就表示这个字节后面还有其他字节，如果这个位是0的话，就表示这是最后一个字节了，这样一来，就可以准确的知道一个整数的结束位置了。&lt;/p&gt;

&lt;p&gt;就以protobuf文档中的整数300为例，先看一下如何将300编码成二进制序列。&lt;/p&gt;

&lt;p&gt;300的二进制表示为：&lt;code&gt;100101100&lt;/code&gt;，显然300这个整数只需要2个字节就可以存储了，根本不需要4个字节。&lt;/p&gt;

&lt;p&gt;第一步：从低位到高位7bit划开，最后不足7bit的剩余部分用0补齐。也就是：&lt;font color='red'&gt;0000010&lt;/font&gt; &lt;font color='blue'&gt;0101100&lt;/font&gt;&lt;/p&gt;

&lt;p&gt;第二步：反转字节顺序。结果：&lt;font color='blue'&gt;0101100&lt;/font&gt; &lt;font color='red'&gt;0000010&lt;/font&gt;。这其实就是规定了字节序的问题。&lt;/p&gt;

&lt;p&gt;第三步：填充标志位。上一步产生的二进制序列，每个组只有7个bit位，不足8bit，因为还没有加上最高位的标志位。这一步加上标志位后就是：&lt;font color='blue'&gt;10101100&lt;/font&gt; &lt;font color='red'&gt;00000010&lt;/font&gt;。这样就得到了300经过base 128 varint编码后的结果了，2个字节搞定。&lt;/p&gt;

&lt;p&gt;拿到二进制数据进行解码的过程当然就是编码过程的一个逆序了，这个一点难度都没有了。不过，这里的过程是人类自然语言的描述，和实际用计算语言来描述还是存在一点差异的。所以，如果真有一种程序语言可以用自然语言的方式来书写代码，那真的是很牛逼的样子。&lt;/p&gt;

&lt;p&gt;Base 128 Varint编码算法其实挺简单的，简单到我还码这么多文字，都快觉得不好意思了。还是再附带一下代码，方便以后自取。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int encode_varint(char *buf, uint64_t x)
{
    int n;

    n = 0;

    while (x &amp;gt; 127) {
        buf[n++] = (char) (0x80 | (x &amp;amp; 0x7F));
        x &amp;gt;&amp;gt;= 7;
    }

    buf[n++] = (char) x;
    return n;
}

uint64_t decode_varint(char *buf)
{
    int      shift, n;
    uint64_t x, c;

    n = 0;
    x = 0;

    for (shift = 0; shift &amp;lt; 64; shift += 7) {
        c = (uint64_t) buf[n++];
        x |= (c &amp;amp; 0x7F) &amp;lt;&amp;lt; shift;
        if ((c &amp;amp; 0x80) == 0) {
            break;
        }
    }

    return x;
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;用C简单的实现了对uint64_t类型的整数的变长编码和解码的函数，可以看出代码其实是很简单的。这个代码可以再优化一下，实现一个更加通用的版本。&lt;/p&gt;

&lt;p&gt;十一玩开心!&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>一个洗牌场景及算法</title>
   <link href="http://skoo87.github.io/algorithm/2013/09/28/shuffle-algorithm"/>
   <updated>2013-09-28T00:00:00+08:00</updated>
   <id>http://skoo87.github.io/algorithm/2013/09/28/shuffle-algorithm</id>
   <content type="html">&lt;p&gt;那些非常简单的，却又能真正的解决实际问题的算法往往才是最经典的。直观感受，洗牌算法运用最多的应该是在游戏开发中，其实并不一定，洗牌其实就是排序算法的反向操作，目的就是将原本可能存在的一定顺序给打乱，变成一种非期望的顺序，所以应用场景应该还是蛮广泛的。简单的打乱一组次序应该是一件很容易的事情，但一个正确的洗牌算法除了能够打乱次序外，还具备等概率性，也就是说任何一个元素被打乱到任何一个位置的概率是相等的。&lt;/p&gt;

&lt;p&gt;其实，我平时的编程中好像很少碰到洗牌的时候。最近在Go语言的实现中看到一个有趣的洗牌场景，这里记录一下和大家分享。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;select {
case 条件1:
	。。。
case 条件2:
	。。。
case 条件3:
	。。。
case 条件4:
	。。。
case 条件5:
	。。。
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你了解Go语言的话，应该知道上面的select语句会&lt;strong&gt;随机&lt;/strong&gt;的选择一个&lt;strong&gt;可以执行&lt;/strong&gt;的case条件。这里是如何实现随机选择的呢？？？&lt;/p&gt;

&lt;p&gt;有人说，“这个很简单啊，利用随机数在这5个条件中随机选择一个就可以了，只要随机数发生器实现得好就行了”。 这个方法看上去似乎可以工作，但这5个条件并不是都可以执行的，可以理解为有的条件并不是true。select的目的是在为true的条件中随机选择一个，并不是在所有条件中进行随机选择。&lt;/p&gt;

&lt;p&gt;既然是在可执行的条件中随机选择一个来执行，那么我们再加一个步骤，“首先遍历这里的所有case条件，把可执行的条件给标记出来，然后再在标记出来的可执行条件中利用随机数来随机选择一个”。嘿，这方法可以真正解决问题了，行得通的样子。其实，在Go runtime里，使用这个方法去做，挺麻烦的，主要的问题是判断条件是否可执行，因为并不存在一个判断的过程，而是直接去执行，执行了就ok了，不能够执行就算了，所以针对条件是一个试图去执行的探测，之前并没有一个可判断的标准。&lt;/p&gt;

&lt;p&gt;Go语言的实现，在解决这个问题的时候，就利用了洗牌，本文也就是介绍洗牌算法用于这种场景。因为在试图执行每个case条件之前，并不知道哪些条件可执行，所以Go的实现者就先把所有的条件利用洗牌给重新排序一遍，然后再从头到尾依次试图执行每个条件，碰到一个可执行条件后，就忽略后面的条件了。这样子一来，就做到了在所有可执行条件中，随机选择一个可执行的条件。这种场景利用洗牌来实现，明显是非常的合适和优雅的。&lt;/p&gt;

&lt;p&gt;常用的洗牌算法蛮多的，比如：经典的&lt;a href='http://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle'&gt;Fisher-Yates&lt;/a&gt;，伪代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;To shuffle an array a of n elements (indices 0..n-1):
for i from n − 1 downto 1 do
   j ← random integer with 0 ≤ j ≤ i
   exchange a[j] and a[i]&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;本文提到的Go采用的洗牌算法和Fisher-Yates非常像，甚至我觉得它就是Fisher-Yates，只是和这个伪代码有一小点不同之处。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for(i=1; i&amp;lt;sel-&amp;gt;ncase; i++) {
    o = sel-&amp;gt;pollorder[i];
    j = runtime·fastrand1()%(i+1);
    sel-&amp;gt;pollorder[i] = sel-&amp;gt;pollorder[j];
    sel-&amp;gt;pollorder[j] = o;
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看出这两个洗牌算法非常相似，但又有一点不同。因此，我决定对Go的洗牌算法进行统计测试，测试方法参考：&lt;a href='http://coolshell.cn/articles/8593.html'&gt;如何测试洗牌程序&lt;/a&gt;。我的测试是执行10000次洗牌，测试代码：&lt;a href='https://gist.github.com/skoo87/6724116#file-go_shuffle-go'&gt;https://gist.github.com/skoo87/6724116#file-go_shuffle-go&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;结果数据：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	 0     1     2 	   3 	 4     5     6     7     8     9    （位置）
	     
1 -&amp;gt;   951  1010   980  1016  1036  1007  1048   951  1020   981
5 -&amp;gt;   940  1041  1028  1057   997  1038   979   991   923  1006
3 -&amp;gt;  1041   976  1000   994   962  1009   970   979  1026  1043
9 -&amp;gt;  1013   964  1021   957   971   983  1034  1052  1016   989
7 -&amp;gt;  1053  1009  1031   960  1012   998   970  1022   993   952
6 -&amp;gt;  1012   963   984   980  1011  1000  1034   950  1041  1025
2 -&amp;gt;   986   992   994   975   993  1025  1023   994  1025   993
8 -&amp;gt;  1001   986  1006   992   996   952   996  1043  1003  1025
0 -&amp;gt;   980  1041  1003  1043   979  1003   981   995   961  1014
4 -&amp;gt;  1023  1018   953  1026  1043   985   965  1023   992   972	
(被洗对象)&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从测试结果数据来看，每个对象出现在每个位置的次数相差不大，还是很靠谱的。&lt;/p&gt;

&lt;p&gt;可以看出这分数据是一个二维表的形式，其实，我很想将这份数据给绘制成图表，最理想的图就是构造一个二维坐标，每个对象出现的位置就画一个点，那么最后形成的图看上去各个地方的密度都差不多的时候就说明洗牌算法比较靠谱。不知道有什么好用的工具可以直观的可视化这种形式的数据，如果你知道，麻烦告诉我一下。&lt;/p&gt;

&lt;p&gt;今天天气不怎么好，宅家里码字比较开心，因为不觉得是在浪费美好的时间，哈哈。&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>Go语言select的内存分配情况</title>
   <link href="http://skoo87.github.io/go/2013/09/28/go-select-memory-test"/>
   <updated>2013-09-28T00:00:00+08:00</updated>
   <id>http://skoo87.github.io/go/2013/09/28/go-select-memory-test</id>
   <content type="html">&lt;p&gt;最近看了Go runtime中关于select的实现（&lt;a href='http://www.bigendian123.com/go/2013/09/26/go-runtime-select/'&gt;select in Go&amp;#8217;s runtime&lt;/a&gt;），发现select语句位于for循环之内执行的时候，每一遍循环都需要在底层runtime中经历malloc对象到free对象的过程，我认为这个频繁的内存分配和释放的代价并不小，至少内存不是处于一种稳定的状态。因此，我实际的测试一把&lt;strong&gt;使用select来操作channel&lt;/strong&gt;和&lt;strong&gt;不使用select操作channel&lt;/strong&gt;两种情况下的内存情况。&lt;/p&gt;

&lt;p&gt;测试过程都是运行程序3分钟，每一次循环sleep 1秒钟，每10秒钟采集一下内存使用情况的数据。为了更直观的感受，我使用&lt;a href='https://github.com/skoo87/goplot'&gt;goplot&lt;/a&gt;工具把采集到的内存数据绘制成了图表。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;使用select&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;测试代码：&lt;a href='https://gist.github.com/skoo87/6727151#file-test_channel_select-go'&gt;https://gist.github.com/skoo87/6727151#file-test_channel_select-go&lt;/a&gt;&lt;/p&gt;
&lt;img height='350' src='/assets/images/test_select.png' width='500' /&gt;
&lt;p&gt;&lt;strong&gt;不使用select&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;测试代码：&lt;a href='https://gist.github.com/skoo87/6727151#file-test_channel-go'&gt;https://gist.github.com/skoo87/6727151#file-test_channel-go&lt;/a&gt;&lt;/p&gt;
&lt;img height='350' src='/assets/images/test_no_select.png' width='500' /&gt;
&lt;p&gt;上面两图中，最上面的&lt;font color='#3498DB'&gt;蓝色&lt;/font&gt;线表示从系统分配的&lt;code&gt;总的堆内存&lt;/code&gt;大小，中间的&lt;font color='#F39C12'&gt;黄色&lt;/font&gt;线表示&lt;code&gt;空闲的堆内存&lt;/code&gt;大小，最下面的&lt;font color='#D35400'&gt;红色&lt;/font&gt;线表示&lt;code&gt;使用的堆内容&lt;/code&gt;大小。&lt;/p&gt;

&lt;p&gt;两种情况，总分配的堆内存都是一样的，基本没差距，两个测试程序本来就很简单，基本一致。使用的堆内存和空闲的堆内存必然具有此消彼长的关系。这两个数据来看，两种情况有着明显的不同，使用select的时候，由于不断在分配新的内存，所以堆内存的使用一路走高，相应空闲的内存就逐渐变少了。但是不使用select，而是直接操作channel的时候，可以明显感受到内存分配情况是非常稳定的。&lt;/p&gt;

&lt;p&gt;这个测试并不是为了证明select有多么的糟糕，而只说明select将会频繁的分配内存，这里只是简单的使用了两个select，内存的表现还是非常明显的。但我个人并不支持为了不用select，而写出一堆难看的代码，甚至是存在隐患的代码，比如无channel超时。但我们应该避免程序中大量执行select语句。&lt;/p&gt;

&lt;p&gt;最后，用数据，将数据可视化，更能形象直观的展示现象，玩得比较开心。&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>select in Go's runtime</title>
   <link href="http://skoo87.github.io/go/2013/09/26/go-runtime-select"/>
   <updated>2013-09-26T00:00:00+08:00</updated>
   <id>http://skoo87.github.io/go/2013/09/26/go-runtime-select</id>
   <content type="html">&lt;p&gt;select可以用来管理多个channel的读写，以及实现channel读写timeout等。select并不是以库的形式提供，而是语言级支持的语法特性，因此select的实现主要由编译器和runtime共同完成，本文将重点关注runtime部分。&lt;/p&gt;
&lt;img height='350' src='/assets/images/select.png' width='500' /&gt;
&lt;p&gt;select语句的执行主要由4个阶段组成，依次是创建select对象，注册所有的case条件，执行select语句，最后释放select对象。这里提到的select对象是底层runtime维护的一个Select结构，这个对象对Go程序来说基本是透明的。后面的内容中，我将称这个select对象为&lt;strong&gt;&lt;code&gt;选择器&lt;/code&gt;&lt;/strong&gt;。&lt;/p&gt;

&lt;h4 id='select'&gt;选择器内存模型(Select)&lt;/h4&gt;

&lt;p&gt;这里内存模型主要是描述的选择器在内存是如何布局的，是什么样的数据结构来维护的。源码位于runtime/chan.c中，描述内存模型的函数主要是&lt;code&gt;newselect&lt;/code&gt;。newselect就是在内存上创建一个选择器。&lt;/p&gt;

&lt;p&gt;描述选择器内存模型最重要的两个结构体定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;struct	Scase
{
	SudoG	sg;			// must be first member (cast to Scase)
	Hchan*	chan;		// chan
	byte*	pc;			// return pc
	uint16	kind;
	uint16	so;			// vararg of selected bool
	bool*	receivedp;	// pointer to received bool (recv2)
};

struct	Select	
{
	uint16	tcase;			// total count of scase[]
	uint16	ncase;			// currently filled scase[]
	uint16*	pollorder;		// case poll order
	Hchan**	lockorder;		// channel lock order
	Scase	scase[1];		// one per case (in order of appearance)
};&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;Scase&lt;/code&gt;描述Go程序select语句中定义的case条件，也就是说Go程序中的一个case在runtime中就是用Scase这个结构来维护的。可以看到&lt;code&gt;Scase&lt;/code&gt;中有一个&lt;code&gt;Hchan *chan&lt;/code&gt;字段，这个显然就是每个case条件上操作的channel了。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Select&lt;/code&gt;就是定义“选择器”的核心结构了，每个字段当然都很重要，不过可以重点关注pollorder、lockorder、scase三个字段。这里先看一下&lt;code&gt;Scase scase[1]&lt;/code&gt;这个字段的定义，可以猜到scase字段就是用来存储所有case条件的，但这里却只是定义了一个只有一个元素的数组，这怎么够存储多余1个case的情况呢？？？&lt;/p&gt;
&lt;img height='350' src='/assets/images/select1.png' width='500' /&gt;
&lt;p&gt;此图就是整个选择器的内存模型了，这一整块内存结构其实也是由&lt;code&gt;头部结构&lt;/code&gt;+&lt;code&gt;数据结构&lt;/code&gt;组成，头部就是Select那一部分，对应上面提到的&lt;code&gt;struct Select&lt;/code&gt;，数据结构部分都是由数组构成。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;scase&lt;/code&gt;就是一个数组，数组元素为Scase类型，存储每个case条件。&lt;/li&gt;

&lt;li&gt;&lt;code&gt;lockorder&lt;/code&gt;指针指向的也是一个数组，元素为&lt;code&gt;Hchan *&lt;/code&gt;类型，存储每个case条件中操作channel。&lt;/li&gt;

&lt;li&gt;&lt;code&gt;pollorder&lt;/code&gt;是一个uint16的数组.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;从头部开始这一整块内存是由一次类malloc（为什么是类malloc，因为Go有自己的内存管理接口，不是采用的普通malloc）调用分配的，然后再将Select头部结构中的lockorder和pollorder两个指针分别指向正确的位置即可。当然，在一口气分配这块内存前，是事先算好了所有需要的内存的大小的。这里特别强调&lt;strong&gt;一次malloc&lt;/strong&gt;分配所有需要的内存，就是想表达除了C/C++外还有哪门语言有这么强的内存控制能力？其他语言（包括Go）在处理这种情况，手法应该差不多都是先New一个主要的对象，然后New这个主要对象字段中需要的对象。当然，你可能会告诉我，这门语言有很好的内存管理系统，不在乎这样的对象创建&amp;#8230;呵呵。拥有内存的完全控制能力，也是系统软件大量采用C/C++编写的原因，也是其他语言的实现基本采用C/C++的原因吧。这个问题不能继续扯下去了。&lt;/p&gt;

&lt;p&gt;scase字段被定义为1个元素的数组的问题还没有解决。上图展示的是一个有6个case条件的选择器内存模型，可以看到lockorder、pollorder以及scase(&lt;strong&gt;黑色部分&lt;/strong&gt;)都是6个单元的数组。&lt;strong&gt;注意&lt;/strong&gt;，黑色部分的scase的第一个单元位于Select头部结构内存空间中，这个单元就是&lt;code&gt;struct Select&lt;/code&gt;中定义的那个只有一个元素的scase数组了，在malloc分配这块内存的时候，scase就只需要少分配一个单元就可以了，所以上图中可以看出只是多加了5个scase的存储单元。这样一来，scase字段和lockorder、pollorder就没有什么两样了，殊途同归。其实，&lt;code&gt;Scase scase[1]&lt;/code&gt;字段完全可以定义为&lt;code&gt;Scase *scase&lt;/code&gt;嘛，但这样要多浪费一个指针的内存空间。我还是很倾向这种扣字节式的实现方式。&lt;/p&gt;

&lt;p&gt;在newselect函数中创建好这块内存空间后，就再也找不到填充scase、lockorder和pollorder三个数组的过程了，也就是创建好内存模型就结束了，还没填数据呢，这是怎么回事？填充选择器其实就是注册case的过程。&lt;/p&gt;

&lt;p&gt;到这里，选择器就被创建好了，剩下的就是选择器如何工作了。&lt;/p&gt;

&lt;h4 id='case'&gt;注册case条件&lt;/h4&gt;

&lt;p&gt;了解了选择器的内存布局，也就是创建好了一个选择器，再看如何把所有case条件数据注册到选择器中，重点看一下两个函数吧:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;static void
selectsend(Select *sel, Hchan *c, void *pc, void *elem, int32 so)
{
	i = sel-&amp;gt;ncase;
	……………..
	sel-&amp;gt;ncase = i+1;
	cas = &amp;amp;sel-&amp;gt;scase[i];

	cas-&amp;gt;pc = pc;
	cas-&amp;gt;chan = c;
	cas-&amp;gt;so = so;
	cas-&amp;gt;kind = CaseSend;
	cas-&amp;gt;sg.elem = elem;
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个selectsend函数就是在碰到case条件是写数据到channel的时候会调用。它会将Go程序中此case上的数据以及channel等信息传给选择器，填充在具体的Scase结构中，完成写channel的case注册。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;static void
selectrecv(Select *sel, Hchan *c, void *pc, void *elem, bool *received, int32 so)
{
	i = sel-&amp;gt;ncase;
	sel-&amp;gt;ncase = i+1;
	cas = &amp;amp;sel-&amp;gt;scase[i];
	cas-&amp;gt;pc = pc;
	cas-&amp;gt;chan = c;

	cas-&amp;gt;so = so;
	cas-&amp;gt;kind = CaseRecv;
	cas-&amp;gt;sg.elem = elem;
	cas-&amp;gt;receivedp = received;
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;selectrecv和selectsend很像，它是在碰到case条件是从一个channel读取数据的时候会被调用，以完成对读channel的case注册。同样，也是事先将channel以及存放数据的内存传递给选择器，填充在一个Scase中。这里由于是等待读取数据，所以是把存储数据的内存地址交给选择器，然后选择器在从channel取到数据后，将数据拷贝到这个内存里。&lt;/p&gt;

&lt;p&gt;case条件的注册过程特别简单，没什么复杂的内容，但这部分其实和编译器很相关，比如只有一个case的select可以优化成直接操作channel等。&lt;/p&gt;

&lt;h4 id='id13'&gt;执行选择器&lt;/h4&gt;

&lt;p&gt;这部分是select的核心，主要包含选择器是如何管理case条件，如何读写对应的channel。&lt;/p&gt;

&lt;p&gt;选择器和channel的交互是由&lt;code&gt;selectgo()&lt;/code&gt;这个函数实现的，这个函数有一点小长，不过过程其实很简单的。下面贴一个此函数的代码骨架。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;static void*
selectgo(Select **selp)
{
	sel = *selp;

	// 这里是一个很重要的地方，pollorder数组依次填上每个case的编号[0, n]，然后第二个for就
	// 是一个洗牌操作了，将pollorder数组中的编号给随机打乱。目的当然就是为了case条件执行的
	// 随机性。
	for(i=0; i&amp;lt;sel-&amp;gt;ncase; i++)
		sel-&amp;gt;pollorder[i] = i;
	for(i=1; i&amp;lt;sel-&amp;gt;ncase; i++) {
		……….
	}

	// 这里又来两个遍历所有case的循环，好蛋疼啊。这次做的事情就是将lockorder中的元素给排序
	// 一下。注意，lockorder数组中的元素是每个case对应的channel的地址。
	for(i=0; i&amp;lt;sel-&amp;gt;ncase; i++) {
		…………..
	}
	for(i=sel-&amp;gt;ncase; i--&amp;gt;0; ) {
		………
	}

	// sellock就是遍历lockorder数组，然后将数组中的每个channel给加上锁，因为后面不知道将
	// 操作哪个channel，干脆就全部给加上好了？？？真暴力啊。上面对lockorder排序的目的也出来
	// 了，就是方便此处加锁的时候，对lockorder中的channel去重。因为两个case完全可能同时操
	// 作同一个channel，所以lockorder中可能存储重复的channel了。
	sellock(sel);

	// 走到这里，总算把准备工作给干完了，将开始真正干活了。
loop:
	// 这个for循环就是按pollorder的顺序去遍历所有case，碰到一个可以执行的case后就中断循
	// 环。pollorder中的编号在初始化阶段就已经被洗牌了，所以是随机挑了一个可以执行的case。
	for(i=0; i&amp;lt;sel-&amp;gt;ncase; i++) {
		o = sel-&amp;gt;pollorder[i];
		cas = &amp;amp;sel-&amp;gt;scase[o];
		…..

		switch(cas-&amp;gt;kind) {
		case CaseRecv:
			……..

		case CaseSend:
			…….
		case CaseDefault:
			……		
		}
	}

	// 没有找到可以执行的case，但有default条件，这个if里就会直接退出了。
	if(dfl != nil) {
		…..
	}

	// 到这里，就是没有找到可以执行的case，也没有default条件的情况了。
	
	// 把当前goroutine给入队到每个case对应的channel的等待队列中去。channel的等待队列在
	// channel实现中已经详细介绍了。
	for(i=0; i&amp;lt;sel-&amp;gt;ncase; i++) {
		………..
		switch(cas-&amp;gt;kind) {
		case CaseRecv:
			enqueue(&amp;amp;c-&amp;gt;recvq, sg);
			break;

		case CaseSend:
			enqueue(&amp;amp;c-&amp;gt;sendq, sg);
			break;
		}
	}
	// 入队完后，就把当前goroutine给挂起等待发生一个可以执行的case为止。这里同时也把所有
	// channel上的加锁给解开了。
	runtime·park((void(*)(Lock*))selunlock, (Lock*)sel, &amp;quot;select&amp;quot;);

	// 当前goroutine被唤醒开始执行了，再次把所有channel加锁。还是暴力。
	sellock(sel);

	// 这一个遍历case的for循环，很有意思。这里就是本次select不会执行的那些case对应的
	// channel给出队当前goroutine。就是不管它们了，已经找到了一个执行的目标case了。
	for(i=0; i&amp;lt;sel-&amp;gt;ncase; i++) {
		cas = &amp;amp;sel-&amp;gt;scase[i];
		if(cas != (Scase*)sg) {
			c = cas-&amp;gt;chan;
			if(cas-&amp;gt;kind == CaseSend)
				dequeueg(&amp;amp;c-&amp;gt;sendq);
			else
				dequeueg(&amp;amp;c-&amp;gt;recvq);
		}
	}

	// 还是没找到case，重新循环执行一遍。这种情况应该是goroutine被其他一些因素给唤醒了。
	if(sg == nil)
		goto loop;

	………..

	// 解锁退出，完成了select的执行了。
	selunlock(sel);
	goto retc;


// 这些goto的tag，都是针对每个case具体操作channel的过程。和channel的实现中介绍的差不多。
asyncrecv:
	………
	goto retc;

asyncsend:
	…………….
	goto retc;

syncrecv:
	………………..
	goto retc;

syncsend:
	……………..

// select执行完退出的时候，不光是释放选择器对象，还会返回pc。这个pc就是本次select执行的case
// 的地址。只有把这个栈地址返回，才能继续执行case条件中的语句。
retc:
	pc = cas-&amp;gt;pc;
	runtime·free(sel);
	return pc;
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;虽然只是一个代码骨架，也挺长的，估计也只有对照源码才能更好的理解了。总之，select的执行逻辑还是有一点小复杂的。初看的时候，不是特别好理解。再总结一下我认为的几个应该知道的地方：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;select语句的执行将会对涉及的所有channel加锁，并不是只加锁需要操作的channel。&lt;/li&gt;

&lt;li&gt;对所有channel加锁之前，存在一个对涉及到的所有channel进行堆排序的过程，目的就是为了去重。&lt;/li&gt;

&lt;li&gt;select并不是直接随机选择一个可执行的case，而是事先将所有case洗牌，再从头到尾选择第一个可执行的case。&lt;/li&gt;

&lt;li&gt;如果select语句是放置在for循环中，长期执行，会不会每次循环都经历选择器的创建到释放的4个阶段？？？我可以明确的告诉你，目前必然是这样子的，所以select的使用是有代价的，还不低。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;select的实现核心部分其实就完了，优化的空间应该还是挺多的。&lt;/p&gt;

&lt;h4 id='id14'&gt;编译器优化&lt;/h4&gt;

&lt;p&gt;虽然不懂编译器，但还是大概扫了一眼编译器中对select的实现部分。大概看一下cmd/gc/select.c代码中的注释，就可以了解到，编译器其实对select有一定的优化。比如你写的select没有任何的case条件，那还创建选择器干嘛呢；再或者你只有一个case条件，这显然可以不用select嘛；即使有一个case+default，也是可以优化为非阻塞的直接操作channel啊。&lt;/p&gt;

&lt;p&gt;这部分编译器相关的优化可以详细看&lt;code&gt;walkselect()&lt;/code&gt;函数。&lt;/p&gt;

&lt;p&gt;注：本文是基于go1.1.2版本代码。&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>channel in Go's runtime</title>
   <link href="http://skoo87.github.io/go/2013/09/20/go-runtime-channel"/>
   <updated>2013-09-20T00:00:00+08:00</updated>
   <id>http://skoo87.github.io/go/2013/09/20/go-runtime-channel</id>
   <content type="html">&lt;p&gt;Go语言有一个非常大的亮点就是支持语言级别的并发。语言级别提供并发编程，究竟有多重要，可能需要你亲自去体会多线程、事件+callback等常见的并发并发编程模型后才能准确的感受到。为了配合语言级别的并发支持，channel组件就是Go语言必不可少的了。官方提倡的一个编程信条——“使用通信去共享内存，而不是共享内存去通信”，这里说的&amp;#8221;通信去共享内存&amp;#8221;的手段就是channel。&lt;/p&gt;

&lt;p&gt;channel的实现位于&lt;a href='http://golang.org/src/pkg/runtime/chan.c'&gt;runtime/chan.c&lt;/a&gt;文件中。&lt;/p&gt;

&lt;h4 id='channel'&gt;channel底层结构模型&lt;/h4&gt;
&lt;img height='450' src='/assets/images/channel.png' width='500' /&gt;
&lt;p&gt;每个channel都是由一个Hchan结构定义的，这个结构中有两个非常关键的字段就是recvq和sendq。recvq和sendq是两个等待队列，这个两个队列里分别保存的是等待在channel上进行读操作的goroutine和等待在channel上进行写操作的goroutine。&lt;/p&gt;

&lt;p&gt;当我们使用make()创建一个channel后，这个channel的大概内存模型就如上图，有一个Hchan结构头部，头部后面的所有内存将被划分为一个一个的slot，每个slot将存储一个元素。slot的个数当然就是make channel时指定的缓冲大小。如果make的channel是无缓冲的，那么这里就没有slot了，就只有Hchan这个头部结构。channel的这个底层实现就是分配的一段连续内存(数组)，不是采用的链表或者其他的什么高级数据结构，事实上做这件事情也不需要高级的数据结构了。&lt;/p&gt;

&lt;p&gt;这里的所有slot形成的数组本身在不移动内存的情况下，是无法做到FIFO的，事实上，Hchan中还有两个关键字段recvx和sendx，在它们的配合下就将slot数组构成了一个循环数组，就这样利用数组实现了一个循环队列。&lt;/p&gt;

&lt;p&gt;这里得吐槽一小段代码，这段代码就是在make一个channel的函数中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#define	MAXALIGN	7

Hcan *c;

// calculate rounded size of Hchan
n = sizeof(*c);
while(n &amp;amp; MAXALIGN)
	n++;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里的while循环就是要将Hchan结构的大小向上补齐到8的倍数，这样后面的内存空间就是按8字节对齐了。为了完成这个向上的补齐操作，最坏情况要执行7次循环，而事实上是可以一步到位的补齐到8的倍数，完全没必要一次一次的加1进行尝试。这个细节其实在很多代码里都有，Nginx就做得很优雅。我是想说Go的部分代码还是挺奔放的，我个人很不喜欢runtime里面的一些函数/变量的命名。&lt;/p&gt;

&lt;h4 id='channel'&gt;写channel&lt;/h4&gt;

&lt;p&gt;有了channel的底层结构模型，基本上也能想象一个元素是如何在channel进行&amp;#8221;入队/出队&amp;#8221;了。完成写channel操作的函数是&lt;code&gt;runtime·chansend&lt;/code&gt;，这个函数同时实现了同步/异步写channel，也就是带/不带缓冲的channel的写操作都是在这个函数里实现的。同步写，还是异步写，其实就是判断是否有slot。这里叙述一下写channel的过程，不再展示代码了。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;加锁，锁住整个channel结构（就是上面的贴图模型）。加锁是可以理解，只是这个锁也够大的。所以，是否一定总是通过“通信来共享内存”是需要慎重考虑的。这把锁可以看出，channel很多时候不一定有直接对共享变量加锁效率高。&lt;/li&gt;

&lt;li&gt;现在已经锁住了整个channel了，可以开始干活了。判断是否有slot(是否带缓冲)，如果有就做异步写，没有就做同步写。&lt;/li&gt;

&lt;li&gt;假设第2步判断的是&lt;strong&gt;同步写&lt;/strong&gt;，那么就试着从&lt;code&gt;recvq&lt;/code&gt;等待队列里取出一个等待的goroutine，然后将要写入的元素直接交给(拷贝)这个goroutine，然后再将这个拿到元素的goroutine给设置为ready状态，就可以开始运行了。到这里并没有完，如果&lt;code&gt;recvq&lt;/code&gt;里，并没有一个等待的goroutine，那么就将待写入的元素保存在当前执行写的goroutine的结构里，然后将当前goroutine入队到&lt;code&gt;sendq&lt;/code&gt;中并被挂起，等待有人来读取元素后才会被唤醒。这个时候，同步写的过程就真的完成了。&lt;/li&gt;

&lt;li&gt;假设第2步判断的是&lt;strong&gt;异步写&lt;/strong&gt;，异步写相对同步写来说，依赖的对象不再是是否有goroutine在等待读，而是缓冲区是否被写满（是否还有slot）。因此，异步写的过程和同步写大体上也是一样的。首先是判断是否还有slot可用，如果没有slot可用了，就将当前goroutine入队到&lt;code&gt;sendq&lt;/code&gt;中并被挂起等待。如果有slot可用，就将元素追加到一个slot中，再从&lt;code&gt;recvq&lt;/code&gt;中试着取出一个等待的goroutine开始进行读操作(如果recvq中有等待读的goroutine的话)。到这里，异步写也就完成了。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;异步写和同步写在逻辑过程上基本是相同的，只是依赖的对象不一样而已。同步写依赖是否有等待读的goroutine，异步写依赖是否有可用的缓冲区。&lt;/p&gt;

&lt;h4 id='channel'&gt;读channel&lt;/h4&gt;

&lt;p&gt;我们知道了写过程的逻辑，试着推测一下读过程其实一点也不难了。有了写，本质上就有了读了。完成读channel操作的函数是&lt;code&gt;runtime·chanrecv&lt;/code&gt;, 下面简单的叙述一下读过程。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;同样首先加锁，锁住整个channel好干活。&lt;/li&gt;

&lt;li&gt;a通过是否带缓冲来判断做同步读还是异步读, 类似写过程。&lt;/li&gt;

&lt;li&gt;假设是同步读，就试着从&lt;code&gt;sendq&lt;/code&gt;队列取出一个等待写的goroutine，并把需要写入的元素拿过来(拷贝)，再将取出的goroutine给ready起来。如果&lt;code&gt;sendq&lt;/code&gt;中没有等待写的goroutine，就只能把当前读的goroutine给入队到&lt;code&gt;recvq&lt;/code&gt;并被挂起了。&lt;/li&gt;

&lt;li&gt;假设是异步读，这个时候就是判断缓冲区中是否有一个元素，没的话，就是将当前读goroutine给入队到&lt;code&gt;recvq&lt;/code&gt;并被挂起等待。如果有元素的话，当然就是取出最前面的元素，同时试着从&lt;code&gt;sendq&lt;/code&gt;中取出一个等待写的goroutine唤醒它。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;通过读写过程可以看出，读和写是心心相惜的，里面有一个非常重要的细节——读需要去&amp;#8221;唤醒&amp;#8221;写的goroutine，写的时候需要去“唤醒”读的goroutine。所以这里的读写过程其实是成对出现，配合完成工作的，缺少一个都不行。(我好像在说废话)&lt;/p&gt;

&lt;h4 id='channel'&gt;无限大channel的实现&lt;/h4&gt;

&lt;p&gt;有同事提到如何实现一个不限制缓冲区大小的channel，同时还支持select操作。select的实现，放下一次讨论了。不管用什么语言，要实现一个无限制大小的channel，应该都不难。在目前channel的基础如何实现一个无限制大小的channel，在这里我大概说一下我的想法，抛砖引玉。&lt;/p&gt;

&lt;p&gt;现在的channel其实就一个数组而已，为了避免内存拷贝，可以在目前的基础上加一层链表结构。这样一来，只要缓冲区用完后，就可以分配一个新的slot数组，并且和老的数组给链起来构成一个更大的缓冲区。这里代码上最复杂的应该是元素被读走后，需要将空的数组给释放掉。加入链表来构造无限制的channel实现看上去是一种比较简单有效的方案。&lt;/p&gt;

&lt;p&gt;如果channel是无限制缓冲大小的，那么写入的goroutine就永远不会被挂起等待了，也就不要&lt;code&gt;sendq&lt;/code&gt;队列了。当然，没消费者或者消费者挂掉的话，这个channel最终也会导致内存爆掉。所以，无限制大小的channel是否真的有必要？？？&lt;/p&gt;

&lt;p&gt;了解了channel的底层实现，应该可以更好选择“通信去共享内存，还是共享内存去通信”，没有什么是银弹。&lt;/p&gt;

&lt;p&gt;注：本文是基于go1.1.2版本代码。&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>使用github管理我的Vim</title>
   <link href="http://skoo87.github.io/vim/2013/09/18/vim-plugin-manager"/>
   <updated>2013-09-18T00:00:00+08:00</updated>
   <id>http://skoo87.github.io/vim/2013/09/18/vim-plugin-manager</id>
   <content type="html">&lt;p&gt;自从把Vim的配置和插件全部管理在github上后，幸福指数直线飙升，再也不用担心丢掉vim的配置了。当然，你们不用告诉我，Emacs有xxx更好的，再好我也不会叛变的，嘿嘿。先说一下，以前的vim插件官网被墙了，现在有&lt;a href='http://vim-scripts.org/'&gt;http://vim-scripts.org/&lt;/a&gt;可用，基本同步了所有插件。这里，必须得再感谢一下gmarik开发的&lt;a href='https://github.com/gmarik/vundle'&gt;vundle&lt;/a&gt;，vundle是一个vim插件管理工具，这货太方便了，太实用了，推荐每一个Vimer使用。&lt;/p&gt;

&lt;p&gt;管理Vim，主要就是管理Vim配置文件和所有安装的插件。我的所有插件现在都是来自于vim-scripts.org和github，这样一来我就不用再自己download插件，解压到.vim目录了，而是直接用vundle工具从网上自动安装我需要的插件。&lt;/p&gt;

&lt;h4 id='id12'&gt;我的插件管理&lt;/h4&gt;

&lt;p&gt;插件实在太多，想用vundle来自动管理插件，就得先手工安装vundle(别告诉我，这你都懒得装)，然后在.vimrc文件里配置好vundle工具以及想要的插件名字，我使用到的所有插件如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot; ----------------------------------------------------------------------------
&amp;quot;                           vundle, 插件管理
&amp;quot; ----------------------------------------------------------------------------
set nocompatible
filetype off
set rtp+=~/.vim/bundle/vundle/
call vundle#rc()
Bundle &amp;#39;gmarik/vundle&amp;#39;


&amp;quot; github 库
&amp;quot;
&amp;quot; Go 语言插件
Bundle &amp;#39;skoo87/go.vim&amp;#39;

&amp;quot; vim里面支持shell终端
Bundle &amp;#39;skoo87/vimproc&amp;#39;
Bundle &amp;#39;skoo87/vimshell&amp;#39;

&amp;quot; c/c++项目工程插件
Bundle &amp;#39;skoo87/p&amp;#39;

&amp;quot; 书签插件
Bundle &amp;#39;skoo87/bookmarking.vim&amp;#39;

&amp;quot; 主要提供 xolox#shell#execute() 后台执行外部命令的接口
Bundle &amp;#39;vim-scripts/shell.vim--Odding&amp;#39;

&amp;quot; vim-scripts 库
&amp;quot;
Bundle &amp;#39;taglist.vim&amp;#39;
Bundle &amp;#39;OmniCppComplete&amp;#39;
Bundle &amp;#39;The-NERD-tree&amp;#39;
Bundle &amp;#39;SuperTab&amp;#39;
Bundle &amp;#39;a.vim&amp;#39;
Bundle &amp;#39;c.vim&amp;#39;
Bundle &amp;#39;genutils&amp;#39;
Bundle &amp;#39;grep.vim&amp;#39;
Bundle &amp;#39;SudoEdit.vim&amp;#39;

&amp;quot; NOTE: 自己修改了plugin/lookupfile.vim中的快捷键为F4, 默认F5.
Bundle &amp;#39;lookupfile&amp;#39;

Bundle &amp;#39;unite.vim&amp;#39;
Bundle &amp;#39;desertEx&amp;#39;
Bundle &amp;#39;CSApprox&amp;#39;

filetype plugin indent on&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我实在懒得解释这个配置的意思，其实不需要过多的解释，vundle的文档介绍的足够的详细。我的配置做的事情主要就是将我需要的插件名字全部写到配置文件中，并不需要我去一个一个的download下来。&lt;/p&gt;

&lt;p&gt;到这里，配置已经搞好了，这个时候只需要打开Vim，执行命令&lt;code&gt;:BundleInstall&lt;/code&gt;就可以自动安装好所有插件了，是不是很赞啊？我实在觉得特别赞啊。vundle还可以搜索插件，卸载插件等等。&lt;/p&gt;

&lt;h4 id='vimrc'&gt;我的.vimrc配置文件管理&lt;/h4&gt;

&lt;p&gt;经过vundle来自动管理插件后，可以看出.vimrc文件显得更加的重要了，其实我们只需要把这个文件拷贝到另外一台机器，然后打开vim执行&lt;code&gt;:BundleInstall&lt;/code&gt;就完成了整个Vim的安装配置，彻底把我们解放出来、不再折腾了。&lt;/p&gt;

&lt;p&gt;这里，我也将.vimrc文件push到github保存。我在github上建了一个项目叫&lt;code&gt;vimrc&lt;/code&gt;，这个项目里有一个文件也叫&lt;code&gt;vimrc&lt;/code&gt;, 这个文件就是我的.vimrc。然后我将这个vimrc clone到本地，再在home目录下创建一个&lt;code&gt;.vimrc&lt;/code&gt;的&lt;code&gt;软连接&lt;/code&gt;指向从github clone下来的配置文件。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/Users/marckywu/.vimrc -&amp;gt; /Users/marckywu/local/vimrc/vimrc&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从此过来，我只要修改了.vimrc配置文件，就把修改push到github上。我真的是再也不用为vim配置操心了。&lt;/p&gt;

&lt;p&gt;最后，再次感谢vim-scripts和vundle两个项目，真的是利天下。&lt;/p&gt;

&lt;p&gt;不过，用Vim和Emacs真的效率会高很多吗？扯淡。反正我是因为在Linux/Mac上找不到一个轻量的IDE才用Vim的，都是被逼的。&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>timer in Go's runtime</title>
   <link href="http://skoo87.github.io/go/2013/09/12/go-runtime-timer"/>
   <updated>2013-09-12T00:00:00+08:00</updated>
   <id>http://skoo87.github.io/go/2013/09/12/go-runtime-timer</id>
   <content type="html">&lt;p&gt;我们总是使用sleep()类函数来让线程暂停一段时间，在Go语言里，也是使用Sleep()来暂停goroutine。 那么Go语言的sleep究竟是如何现实的呢？当然你翻看标准库中的time包里面的sleep.go源码时， 你可能会觉得看不明白，因为支持sleep功能的真正实现是在runtime里面。不难想到sleep功能是根据定时器来实现的， 因此接下来看看runtime中的timer究竟长什么样子。&lt;/p&gt;

&lt;p&gt;timer的实现主要位于&lt;a href='http://golang.org/src/pkg/runtime/time.goc'&gt;runtime/time.goc&lt;/a&gt;文件中。&lt;/p&gt;

&lt;h4 id='id10'&gt;主要数据结构&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;struct  Timers
{
    Lock;
    G       *timerproc;
    bool    sleeping;
    bool    rescheduling;
    Note    waitnote;
    Timer   **t;
    int32   len;
    int32   cap;
};

struct  Timer
{
    int32   i;      // heap index

    // Timer wakes up at when, and then at when+period, ... (period &amp;gt; 0 only)
    // each time calling f(now, arg) in the timer goroutine, so f must be
    // a well-behaved function and not block.
    int64   when;
    int64   period;
    FuncVal *fv;
    Eface   arg;
};&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;这两个结构是定义在runtime.h文件中。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;调用一次sleep其实就是生成一个&lt;code&gt;Timer&lt;/code&gt;，然后添加到&lt;code&gt;Timers&lt;/code&gt;中。可以看出来Timers就是维护所有Timer的一个集合。除了可以向Timers中添加Timer外，还要从Timers中删除超时的Timer。所以，Timers采用小顶堆来维护，小顶堆是常用来管理定时器的结构，有的地方也使用红黑树。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;code&gt;Timers&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Timers结构中有一个&lt;code&gt;Lock&lt;/code&gt;, 大概猜测一下就知道是用来保护&lt;code&gt;添加/删除&lt;/code&gt;Timer的，实际上也是干这件事的。&lt;/li&gt;

&lt;li&gt;&lt;code&gt;timerproc&lt;/code&gt;指针维护的是一个goroutine，这个goroutine的主要功能就是检查小顶堆中的Timer是否超时。当然，超时就是删除Timer，并且执行Timer对应的动作。&lt;/li&gt;

&lt;li&gt;&lt;code&gt;t&lt;/code&gt;显然就是存储所有Timer的堆了。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;省略几个字段放到下文再介绍。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;code&gt;Timer&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;when&lt;/code&gt;就是定时器超时的时间&lt;/li&gt;

&lt;li&gt;&lt;code&gt;fv&lt;/code&gt;和&lt;code&gt;arg&lt;/code&gt;挂载的是Timer超时后需要执行的方法。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;到此，Go语言的定时器大概模型就能想象出来了。其实，所有定时器的实现都大同小异，长得都差不多。&lt;/p&gt;

&lt;h4 id='timerproc_goroutine'&gt;timerproc goroutine&lt;/h4&gt;

&lt;p&gt;上文提到timerproc维护的是一个goroutine，这个goroutine就做一件事情——不断的循环检查堆，删除掉那些超时的Timer，并执行Timer。下面精简一下代码，看个大概主干就足够明白了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;static void
timerproc(void)
{
    for(;;) {
        for(;;) {
            // 判断Timer是否超时
            t = timers.t[0];
            delta = t-&amp;gt;when - now;
            if(delta &amp;gt; 0)
                break;

            // TODO: 删除Timer, 代码被删除

            // 这里的f调用就是执行Timer了
            f(now, arg);
        }

        // 这个过程是，堆中没有任何Timer的时候，就把这个goroutine给挂起，不运行。
        // 添加Timer的时候才会让它ready。
        if(delta &amp;lt; 0) {
            // No timers left - put goroutine to sleep.
            timers.rescheduling = true;
            runtime·park(runtime·unlock, &amp;amp;timers, &amp;quot;timer goroutine (idle)&amp;quot;);
            continue;
        }

        // 这里干的时候就让这个goroutine也sleep, 等待最近的Timer超时，再开始执行上面的循环检查。当然，这里的sleep不是用本文的定时器来实现的，而是futex锁实现。
        // At least one timer pending.  Sleep until then.
        timers.sleeping = true;
        runtime·notetsleep(&amp;amp;timers.waitnote, delta);
        }
    }
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里一定要记住，&lt;code&gt;timerproc&lt;/code&gt;是在一个独立的goroutine中执行的。梳理一下上面代码的过程：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;判断堆中是否有Timer? 如果没有就将&lt;code&gt;Timers&lt;/code&gt;的&lt;code&gt;rescheduling&lt;/code&gt;设置为true的状态，true就代表timerproc goroutine被挂起，需要重新调度。这个重新调度的时刻就是在添加一个Timer进来的时候，会ready这个goroutine。这里挂起goroutine使用的是runtime·park()函数。&lt;/li&gt;

&lt;li&gt;如果堆中有Timer存在，就取出堆顶的一个Timer，判断是否超时。超时后，就删除Timer，执行Timer中挂载的方法。这一步是循环检查堆，直到堆中没有Timer或者没有超时的Timer为止。&lt;/li&gt;

&lt;li&gt;在堆中的Timer还没超时之前，这个goroutine将处于sleep状态，也就是设置&lt;code&gt;Timers&lt;/code&gt;的&lt;code&gt;sleeping&lt;/code&gt;为true状态。这个地方是通过runtime·notesleep()函数来完成的，其实现是依赖futex锁。这里，goroutine将sleep多久呢？它将sleep到最近一个Timer超时的时候，就开始执行。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;维护Timers超时的goroutine干的所有事情也就这么一点，这里除了堆的维护外，就是goroutine的调度了。&lt;/p&gt;

&lt;h4 id='id11'&gt;添加一个定时器&lt;/h4&gt;

&lt;p&gt;另外一个重要的过程就是如何完成一个Timer的添加? 同样精简掉代码，最好是对照完整的源码看。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;static void
addtimer(Timer *t)
{
    if(timers.len &amp;gt;= timers.cap) {
        // TODO 这里是堆没有剩余的空间了，需要分配一个更大的堆来完成添加Timer。
    }

    // 这里添加Timer到堆中.
    t-&amp;gt;i = timers.len++;
    timers.t[t-&amp;gt;i] = t;
    siftup(t-&amp;gt;i);

    // 这个地方比较重要，这是发生在添加的Timer直接位于堆顶的时候，堆顶位置就代表最近的一个超时Timer.
    if(t-&amp;gt;i == 0) {
        // siftup moved to top: new earliest deadline.
        if(timers.sleeping) {
            timers.sleeping = false;
            runtime·notewakeup(&amp;amp;timers.waitnote);
        }
        if(timers.rescheduling) {
            timers.rescheduling = false;
            runtime·ready(timers.timerproc);
        }
    }
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从代码可以看到新添加的Timer如果是堆顶的话，会检查&lt;code&gt;Timers&lt;/code&gt;的sleeping和rescheduling两个状态。上文已经提过了，这两个状态代表timeproc goroutine的状态，如果处于sleeping，那就wakeup它; 如果是rescheduling就ready它。这么做的原因就是通知那个wait的goroutine——&amp;#8221;堆中有一个Timer了&amp;#8221;或者&amp;#8221;堆顶的Timer易主了&amp;#8221;，你赶紧来检查一下它是否超时。&lt;/p&gt;

&lt;p&gt;添加一个Timer的过程实在太简单了，关键之处就是最后的Timers状态检查逻辑。&lt;/p&gt;

&lt;h4 id='sleep'&gt;Sleep()的实现&lt;/h4&gt;

&lt;p&gt;上面的内容阐述了runtime的定时器是如何运行的，那么Go语言又是如何在定时器的基础上实现Sleep()呢？&lt;/p&gt;

&lt;p&gt;Go程序中调用time.Sleep()后将进入runtime，执行下面的代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void
runtime·tsleep(int64 ns, int8 *reason)
{
    Timer t;

    if(ns &amp;lt;= 0)
        return;

    t.when = runtime·nanotime() + ns;
    t.period = 0;
    t.fv = &amp;amp;readyv;
    t.arg.data = g;
    runtime·lock(&amp;amp;timers);
    addtimer(&amp;amp;t);
    runtime·park(runtime·unlock, &amp;amp;timers, reason);
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;sleep原来就是创建一个Timer，添加到Timers中去，最后调用runtime·park()将当前调用Sleep()的goroutine给挂起就完事了。&lt;/p&gt;

&lt;p&gt;关键是，goroutine被挂起后，如何在超时后被唤醒继续运行呢？这里就是Timer中&lt;code&gt;fv&lt;/code&gt;和&lt;code&gt;arg&lt;/code&gt;两个字段挂载的东西来完成的了。此处，fv挂载了&lt;code&gt;&amp;amp;readyv&lt;/code&gt;，看一下readyv的定义:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;static void
ready(int64 now, Eface e)
{
    USED(now);

    runtime·ready(e.data);
}

static FuncVal readyv = {(void(*)(void))ready};&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;readyv其实就是指向了ready函数，这个ready函数就是在Timer超时的时候将会被执行，它将ready被挂起的goroutine。&lt;code&gt;t.arg.data = g;&lt;/code&gt; 这行代码就是在保存当前goroutine了。&lt;/p&gt;

&lt;p&gt;Sleep()实现总结起来就三大步:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;创建一个Timer添加到Timers中&lt;/li&gt;

&lt;li&gt;挂起当前goroutine&lt;/li&gt;

&lt;li&gt;Timer超时ready当前goroutine&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Go语言的定时器实现还是比较清晰的，没有什么繁琐的逻辑。相比，其他地方(如:Nginx)的实现来说，这里可能就是多了goroutine的调度逻辑。&lt;/p&gt;

&lt;p&gt;看一个东西的实现，重要的是知道作者为何要这样做。只有弄明白了why, how才有价值。&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>Go RPC Benchmark</title>
   <link href="http://skoo87.github.io/go/2013/09/10/go-rpc-benchmark"/>
   <updated>2013-09-10T00:00:00+08:00</updated>
   <id>http://skoo87.github.io/go/2013/09/10/go-rpc-benchmark</id>
   <content type="html">&lt;p&gt;这篇完成得太折腾了，为了更好的展示benchmark的一系列结果数据，我必须得找个软件将数据进行图表化。以前在windows上基本都用execel画曲线图、柱状图等，但在linux/mac上却找不到顺手的工具了。我也使用过gnuplot，这货不知道是太专业，还是太古老的原因，始终用得不顺手、不开心。于是，我就决定自己先用Go和chart.js库折腾了一个&lt;strong&gt;&lt;a href='https://github.com/skoo87/goplot'&gt;goplot&lt;/a&gt;&lt;/strong&gt;工具来绘制图表，然后再才开始写这篇博客。&lt;/p&gt;

&lt;p&gt;有人可能会说我又在折腾轮子了，确实是折腾了一个轮子。话说，这又怎么样呢？作为一个程序员，最大的优势就是自己用得不开心的工具，可以自己动手完善、甚至写一个新的。我认为一个geek程序员首先就是要学会不断的装备自己的工具库。不扯废话了，回归正题。&lt;/p&gt;

&lt;h4 id='id7'&gt;测试维度&lt;/h4&gt;

&lt;p&gt;这次benchmark主要以下两个维度进行测试&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;单个连接上，不同并发数的QPS&lt;/li&gt;

&lt;li&gt;固定100个并发，不同连接数的QPS&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;测试过web server的人可能对这里的并发有一点疑惑。web服务器的并发数一般是指连接数，这里的并发数不是指的连接数，而是同时在向服务器发送请求的goroutine数(理解为线程数)。因为rpc的网络连接是支持并发请求的，也就是一个连接上是可以同时有很多的请求在跑，这不同于http协议一问一答的模式，而是和SPDY协议非常类似。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;维度1&lt;/code&gt; 关注的是一个连接上能跑的请求的极限究竟是多少，这个可以用来粗略的衡量与后端的连接数。毕竟后端系统(如:DB)并不希望存在大量的连接，这和web服务器有一点不同。这个维度更多的也是在衡量rpc的客户端究竟能够发送得多快。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;维度2&lt;/code&gt; 关注的基本就是rpc服务器的性能了，也就是看一个goroutine一个连接后，服务器还能应付多大的并发。&lt;/p&gt;

&lt;h4 id='id8'&gt;测试环境&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Client和Server分别部署在两台机器上，当然两台机器都在同一个局域网内，可以忽略网络。&lt;/li&gt;

&lt;li&gt;Client和Server的机器都是8核cpu超线程到16核，内存和网卡都远远足够。&lt;/li&gt;

&lt;li&gt;Client每个请求只发送一个100bytes的字符串给服务器。&lt;/li&gt;

&lt;li&gt;GOMAXPROCS设置为16&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id='id9'&gt;测试结果&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;单连接上，不同并发数的QPS&lt;/strong&gt;&lt;/p&gt;
&lt;div align='center'&gt;
&lt;img height='350' src='/assets/images/rpc-benchmark-one-conn.png' width='500' /&gt;
&lt;/div&gt;
&lt;p&gt;可以看到随着这个连接上的并发数增大，qps最后稳定在5.5W左右。这个性能有点偏低了。这一块的性能影响主要来自锁的开销、序列化/反序列化、以及goroutine的调度。一个rpc客户端（也就是和服务器一个连接）有一个input goroutine来负责读取服务器的响应，然后分发给每个写入请求的goroutine，如果这个goroutine长时间不能被调度到，那么就会导致发送者的阻塞等待，Go目前的调度明显是会有这个问题的，留到调度器再聊。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;100并发，不同连接数的QPS&lt;/strong&gt;&lt;/p&gt;
&lt;div align='center'&gt;
&lt;img height='350' src='/assets/images/rpc-benchmark-multi-conn.png' width='500' /&gt;
&lt;/div&gt;
&lt;p&gt;多个rpc客户端可以到随着连接数的增多，qps最后可以达到20W左右，这个时候瓶颈主要出现在cpu上了。很久以前，我用同一批机器测试avro的时候100个线程100个连接，qps在14+W。这个维度来看Go的rpc性能非常不错。&lt;/p&gt;

&lt;p&gt;性能应该是一个需要我们持续关注和优化的问题。&lt;/p&gt;

&lt;p&gt;最后附上测试代码: &lt;a href='https://gist.github.com/skoo87/6510680'&gt;https://gist.github.com/skoo87/6510680&lt;/a&gt;&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>Go RPC Inside (server)</title>
   <link href="http://skoo87.github.io/go/2013/09/01/go-rpcserver-inside"/>
   <updated>2013-09-01T00:00:00+08:00</updated>
   <id>http://skoo87.github.io/go/2013/09/01/go-rpcserver-inside</id>
   <content type="html">&lt;p&gt;说到rpc让我想起了刚毕业面试的时候，被问到是否了解rpc？我记得当时我的回答是“课本上学过rpc，只知道是远程过程调用，但没有用过，具体也不知道是什么”。的确，大学中间件这门课程里有讲到rpc，里面还引入了一个非常难理解的概念——“桩”，英文应该叫&amp;#8221;stub&amp;#8221;。现在的rpc实现里，stub这个概念好像都没见到了，应该都是叫&amp;#8221;method&amp;#8221;。&lt;/p&gt;

&lt;p&gt;实现一个rpc服务器很难吗？rpc服务器也就是在tcp服务器的基础上加上自定义的rpc协议而已。一个rpc协议里，主要有个3个非常重要的信息。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;调用的远程method名字，一般就是一个函数名&lt;/li&gt;

&lt;li&gt;call参数，也就是发送给服务器的数据&lt;/li&gt;

&lt;li&gt;客户端生成的调用请求seq&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;除了最后一点，其他两点显然就是组成一个普通的函数调用而已，这也就是远程过程调用了。最后一点的seq只是rpc内部实现的一个关键点而已，也许还有其他的实现方式，而不是依赖seq来保证单连接的并发请求。&lt;/p&gt;

&lt;h4 id='crpc'&gt;如何用C语言实现rpc服务器？&lt;/h4&gt;

&lt;p&gt;用C语言来实现rpc服务器，先假设我们从socket里读取到了一个来自于客户端的call请求，这个call请求里面封装了上面提到的3点信息。&lt;/p&gt;

&lt;p&gt;执行这个call，最重要的就是——&lt;strong&gt;“从call里取出method，也就是一个函数名(字符串)，然后要通过这个函数名去执行对应的函数”。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;C语言由于没有反射机制，于是不能通过函数的名字去调对应的函数；因此，可以使用一张hash表保存所有远程函数的名字到函数的对应关系。这样就可以通过method查找一次hash表得到真正的函数，接下来就可以执行函数了，函数的执行结果当然就是作为响应返回给客户端。&lt;/p&gt;

&lt;p&gt;当然，这些需要被调用执行的函数都是在服务器初始化的时候事先注册到这张hash表中的。到这里，感觉一切都结束了。其实，还有&lt;strong&gt;参数&lt;/strong&gt;(请求数据)和&lt;strong&gt;返回值&lt;/strong&gt;(应答数据)，这部分主要是涉及到序列化算法，留到下次的&lt;em&gt;&amp;#8220;反射和序列化&amp;#8221;&lt;/em&gt;再聊了。&lt;/p&gt;

&lt;h4 id='go_rpc'&gt;使用Go RPC框架写一个简单服务器&lt;/h4&gt;

&lt;p&gt;定义服务&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type EchoServer bool

func (s *EchoServer) Echo(req *string, res *string) error {
	res = req
	return nil
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注册服务&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo := new(EchoServer)
if err := rpc.Register(echo); err != nil {
	return err
}

// 下面基本是固定代码，创建一个tcp服务器
var listener *net.TCPListener
if tcpAddr, err := net.ResolveTCPAddr(&amp;quot;tcp&amp;quot;, addr); err != nil {
	return err
} else {
	if listener, err = net.ListenTCP(&amp;quot;tcp&amp;quot;, tcpAddr); err != nil {
		return err
	}
}

for {
	conn, err := listener.Accept()
	if err != nil {
		sc.LOG.Error(err)
		continue
	}
	// 服务器走rpc框架的入口，也就是在一个tcp连接上采用rpc协议来处理请求和应答。
	go rpc.ServeConn(conn)
}
return nil&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样，就实现了一个简单的echo服务器，功能逻辑都在自定义的EchoServer里面实现。接下来只需要将EchoServer的一个实例对象注册到rpc框架中，客户端就可以直接调用EchoServer对象中的方法了。&lt;/p&gt;

&lt;p&gt;Go rpc服务器的内部实现在思路上和前文提到的C语言实现基本是一样的。细心的人可能注意到了，这里注册的是EchoServer对象，而不是EchoSever对象的方法，然后前文的C语言实现中注册的直接是函数，那么rpc服务器如何能够根据&lt;code&gt;method&lt;/code&gt;去调用&lt;code&gt;对应的方法&lt;/code&gt;呢？Go语言在这里其实采用反射的手段，虽然表面上是注册的EchoServer对象，实际却是通过反射取得了EchoServer的所有方法，然后采用了map表保存了method到方法的映射，这就回到了前文C语言的实现思路中去了。如果没有反射的支持，就只能一个一个的方法全部注册一遍了，并且代码组织上也不够优雅。&lt;/p&gt;

&lt;h4 id='id5'&gt;编码解码器&lt;/h4&gt;

&lt;p&gt;rpc客户端有一个编码解码器定义了如何发送请求和读取应答，那么服务器端必然有一个编码解码器定义了如何读取请求和发送应答，刚好是一个相反的过程，这也就是序列化和反序列化的一个用处。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ServerCodec interface {
	ReadRequestHeader(*Request) error
	ReadRequestBody(interface{}) error
	WriteResponse(*Response, interface{}) error

	Close() error
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;和客户端一样，你只需要实现ServerCodec这个接口，就可以自定义服务端的编码解码器，实现自定义的rpc协议了。 Go rpc服务器端和客户端都是默认使用的Gob序列化协议数据。Go里面为什么采用自实现的Gob，而不是Google的protobuf，这个也留到下次的&lt;em&gt;&amp;#8220;反射和序列化&amp;#8221;&lt;/em&gt;聊吧。&lt;/p&gt;

&lt;p&gt;注意，ServerCodec接口中的&lt;code&gt;ReadRequestBody(interface{}) error&lt;/code&gt;方法主要是用来读取客户端call请求的参数数据，也就是将socket中读出来的数据包解码到&lt;code&gt;interface{}&lt;/code&gt;所指向的具体对象中去，这里的&lt;code&gt;interface{}&lt;/code&gt;可以理解为C语言中的&lt;code&gt;void *&lt;/code&gt;。你一定注意到了，不同的rpc服务定义的参数类型完全不同，并且在rpc框架内部都采用了&lt;code&gt;interface{}&lt;/code&gt;来适配，那么框架内部如何知道读取的socket数据要解码到什么具体类型中去呢？这里又是涉及到了反射，因为有了反射就可以从&lt;code&gt;interface{}&lt;/code&gt;得到具体的类型。&lt;/p&gt;

&lt;p&gt;接下来真得好好的说说&lt;em&gt;“反射和序列化/反序列化”&lt;/em&gt;了。&lt;/p&gt;

&lt;h4 id='id6'&gt;几个内部细节&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;服务器在发送应答的时候，同样采用了一把锁来保证原子性写入socket&lt;/li&gt;

&lt;li&gt;请求/应答结构(ServerCodec接口中出现的&lt;code&gt;Request/Response&lt;/code&gt;)采用链表实现一个pool，需要一个Requet/Response的时候都从free list中获取，不再频繁的分配。这一点只是一个优化。&lt;/li&gt;

&lt;li&gt;Go的rpc除了直接跑在tcp服务器上，还可以跑在更高层一点的http服务器上。&lt;/li&gt;

&lt;li&gt;Go的rpc调用也提供了json序列化。&lt;/li&gt;
&lt;/ul&gt;
&lt;br /&gt;
&lt;p&gt;准备改变一下以前写技术文章老是大段大段代码的风格，试试小代码能不能将自己的想法交代清楚，哈哈。这一篇就先到此为止了。&lt;/p&gt;

&lt;p&gt;下一篇就看一下Go RPC框架的性能，做一个benchmark比较下。&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>Go RPC Inside (client)</title>
   <link href="http://skoo87.github.io/go/2013/08/28/go-rpcclient-inside"/>
   <updated>2013-08-28T00:00:00+08:00</updated>
   <id>http://skoo87.github.io/go/2013/08/28/go-rpcclient-inside</id>
   <content type="html">&lt;p&gt;Go语言标准库能够自带一个rpc框架还是非常给力的，这可以很大程度的降低写后端网络通信服务的门槛，特别是在大规模的分布式系统中，rpc基本是跨机器通信的标配。rpc能够最大程度屏蔽网络细节，让开发者专注在服务功能的开发上面。下面介绍Go语言rpc框架的客户端内部实现.&lt;/p&gt;

&lt;p&gt;Go rpc客户端的逻辑很简单，大体上，就是将一个个的调用请求序列化后原子的发送给服务器，然后有一个专门的gorutine等待服务器应答，这个goroutine会将收到的每个应答分发给对应的请求，这就完成了一次rpc调用。&lt;/p&gt;

&lt;h4 id='id1'&gt;调用入口&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;func NewClient(conn io.ReadWriteCloser) *Client

func (client *Client) Call(serviceMethod string, args interface{}, reply interface{}) error&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用rpc客户端，首先要NewClient得到一个客户端对象，然后通过这个客户端对象的Call方法去执行远程服务端方法。这里需要注意，NewClient函数不光是创建了一个客户端对象，同时还创建了一个input goroutine. input goroutine是阻塞在连接上，等待服务端的响应。这里Call方法的主要工作是将一个rpc请求发送到服务端，同时放入一个等待队列，等候服务器的响应到来。&lt;/p&gt;

&lt;h4 id='id2'&gt;发送请求&lt;/h4&gt;

&lt;p&gt;经过客户端Call方法提交的rpc的请求，最终将调用如下的send方法发送给服务器，这里重点分析一下send的实现。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (client *Client) send(call *Call) {
	//
	// 入口处就来了一把锁，这把锁显然是为了保证一个请求能够被原子的写入socket中. 毕竟一个
	// rpc连接是会被多个goroutine并发写入的，因此这里需要保证发送请求的原子性。
	// 
	client.sending.Lock()
	defer client.sending.Unlock()

	// 
	// 这里又来了一把锁，这把锁的目标是客户端的pending等待队列，也就是将每个rpc请求放入等待队列
	// 的时候，需要对这个pending队列做并发写保护。
	//
	// Register this call.
	client.mutex.Lock()
	if client.shutdown || client.closing {
		call.Error = ErrShutdown
		client.mutex.Unlock()
		call.done()
		return
	}
	// 
	// pending队列其实是使用map实现的，这里可以看到每个rpc请求都会生存一个唯一递增的seq, 这个
	// seq就是用来标记请求的，这个很像tcp包的seq。
	seq := client.seq
	client.seq++
	client.pending[seq] = call
	client.mutex.Unlock()

	// 
	// 下面就是将一个rpc请求的所有数据 (请求方法名、seq、请求参数等)进行序列化打包，然后发送
	// 出去. 这里主要采用的是Go标准库自带的gob算法进行请求的序列化。
	// 
	// Encode and send the request.
	client.request.Seq = seq
	client.request.ServiceMethod = call.ServiceMethod
	err := client.codec.WriteRequest(&amp;amp;client.request, call.Args)
	if err != nil {
		client.mutex.Lock()
		call = client.pending[seq]
		delete(client.pending, seq)
		client.mutex.Unlock()
		if call != nil {
			call.Error = err
			call.done()
		}
	}
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过对发送过程的分析，可以看出一个rpc请求发出去主要有三个过程，第一个过程是将请求放入等待队列，第二个过程就是序列化，最后一个就是写入socket.&lt;/p&gt;

&lt;p&gt;send过程最重要的就是两把锁 + seq。复用连接的原子写、并发请求是实现rpc框架的基础。&lt;/p&gt;

&lt;h4 id='id3'&gt;读取应答&lt;/h4&gt;

&lt;p&gt;前面提到在创建一个rpc客户端对象的时候，同时会启动一个input goroutine来等待服务器的响应，下面的input方法就是这个goroutine做的所有工作。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (client *Client) input() {
	var err error
	var response Response
	//
	// 这里的for循环就是永久负责这个连接的响应读取，只有在连接上发生错误后，才会退出。
	// 
	for err == nil {
		//
		// 首先是读响应头, 响应头一般有一个很重要的信息就是正文数据长度，有了这个长度信息，才知道
		// 读多少正文才是一个应答完毕。
		//
		response = Response{}
		err = client.codec.ReadResponseHeader(&amp;amp;response)
		if err != nil {
			break
		}
		// 
		// 这里是一个很重要的步骤，从响应头里取到seq，这个seq就是客户端生成的seq，在上文的send
		// 过程中发送给了服务器，服务器应答的时候，必须将这个seq响应给客户端。只有这样客户端才
		// 知道这个应答是对应pending队列中的那个请求的。
		// 
		// 这里对pending队列枷锁保护，通过seq提取对应的请求call对象。
		//
		seq := response.Seq
		client.mutex.Lock()
		call := client.pending[seq]
		delete(client.pending, seq)
		client.mutex.Unlock()

		//
		// 这个switch里主要就是处理异常以及正常情况读取响应正文了。异常情况，英文解释很详细了。
		// 
		switch {
		case call == nil:
			// We&amp;#39;ve got no pending call. That usually means that
			// WriteRequest partially failed, and call was already
			// removed; response is a server telling us about an
			// error reading request body. We should still attempt
			// to read error body, but there&amp;#39;s no one to give it to.
			err = client.codec.ReadResponseBody(nil)
			if err != nil {
				err = errors.New(&amp;quot;reading error body: &amp;quot; + err.Error())
			}
		case response.Error != &amp;quot;&amp;quot;:
			// We&amp;#39;ve got an error response. Give this to the request;
			// any subsequent requests will get the ReadResponseBody
			// error if there is one.
			call.Error = ServerError(response.Error)
			err = client.codec.ReadResponseBody(nil)
			if err != nil {
				err = errors.New(&amp;quot;reading error body: &amp;quot; + err.Error())
			}
			call.done()
		default:
			//
			// 开始读取响应的正文，正文放到call中的Reply中去。
			//
			err = client.codec.ReadResponseBody(call.Reply)
			if err != nil {
				call.Error = errors.New(&amp;quot;reading body &amp;quot; + err.Error())
			}
			call.done()
		}
	}
	
	//
	// 下面部分的代码都是在处理连接上出错，以及服务端关闭连接等情况的清理工作. 这部分很重要，
	//  否则可能导致一些调用rpc的goroutine永久阻塞等待，不能恢复工作。
	// 
	// Terminate pending calls.
	client.sending.Lock()
	client.mutex.Lock()
	client.shutdown = true
	closing := client.closing
	if err == io.EOF {
		if closing {
			err = ErrShutdown
		} else {
			err = io.ErrUnexpectedEOF
		}
	}
	for _, call := range client.pending {
		call.Error = err
		call.done()
	}
	client.mutex.Unlock()
	client.sending.Unlock()
	if err != io.EOF &amp;amp;&amp;amp; !closing {
		log.Println(&amp;quot;rpc: client protocol error:&amp;quot;, err)
	}
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;不管有多少goroutine在并发写一个rpc连接，等待在连接上读取应答的goroutine只有一个，这个goroutine负责读取连接上的所有响应，然后将响应分发给对应的请求，也就完成了一次rpc请求了。&lt;/p&gt;

&lt;h4 id='id4'&gt;自定义编码解码器&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;type ClientCodec interface {
	WriteRequest(*Request, interface{}) error
	ReadResponseHeader(*Response) error
	ReadResponseBody(interface{}) error

	Close() error
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ClientCodec定义了编码解码器的接口，也就是说你如果要自定义一个编码解码器，只需要实现这个接口就可以了。&lt;/p&gt;

&lt;p&gt;WriteRequest做的事情就是将rpc请求的远程方法名、参数等信息进行序列化打包，然后写入socket中。你完全可以按照自己的想法去打包一请求进行发送。 两个Read接口就是从socket读取响应数据包，然后采用相应的算法进行反序列化解包即可。&lt;/p&gt;

&lt;p&gt;Go的rpc如果不能自定义编码解码器，那就只能用在客户端和服务器都是Go语言开发的环境中。这就大幅度的降低了其灵活性。能够自定义编码解码器后，理论上就可以用来实现其他rpc的协议，比如：thrift。github的go-thrift项目确实是在Go rpc的基础上通过自定义编码解码器实现的thrift协议。&lt;/p&gt;

&lt;h4 id='timeout'&gt;timeout&lt;/h4&gt;

&lt;p&gt;Call调用并没有提供超时机制，如果服务器响应太慢，怎么办？死等下去吗？这很不合理。有人说可以自己实现Call方法，不使用rpc包默认提供的Call。代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (client *Client) Call(serviceMethod string, args interface{}, reply interface{}) error {
	select {
	case call := &amp;lt;-client.Go(serviceMethod, args, reply, make(chan *Call, 1)).Done:
		return call.Error
	case &amp;lt;-time.After(5*time.Second):
	}
	return errors.New(&amp;quot;timeout&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面这段代码确实可以做到rpc调用5s没有得到响应，就超时放弃等待应答。&lt;/p&gt;

&lt;p&gt;但是，这样做会有另外一个潜在的风险，就是内存泄漏. 因为，client.Go这个方法会构造一个call对象，则个call对象其实就是一个请求，这个请求被发送出去的同时会放到一个pending队列中，上文已经分析过了。通过分析input goroutine的过程可以看到如果服务器不响应的话，这个call对象就一直待在pending队列中了，永远不会被删除掉。当然，这种情况说明我们的服务器设计有问题。&lt;/p&gt;

&lt;p&gt;实现一个rpc客户端，最重要的事情就是连接复用，也就是说一个连接上需要并发的同时跑多个请求。Go rpc在完成这件事情上，动用了两把锁，这个地方很可能会是影响性能的点。待服务器实现分析完后，我们来动手测试一下rpc的性能, 看看究竟如何。&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>example</title>
   <link href="http://skoo87.github.io/eg/1970/01/01/example"/>
   <updated>1970-01-01T00:00:00+08:00</updated>
   <id>http://skoo87.github.io/eg/1970/01/01/example</id>
   <content type="html"></content>
 </entry>
 
 
</feed>